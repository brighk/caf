%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Causal Autonomy Framework Architecture}
\label{ch:caf_architecture}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter presents the Causal Autonomy Framework (CAF), a production-grade neuro-symbolic architecture that integrates large language model reasoning with formal verification and causal validation. Building on the theoretical foundations established in Chapter 3, we describe the complete system design from high-level architectural principles through low-level implementation details.

The chapter is structured as follows: Section~\ref{sec:caf_overview} provides system overview and design principles; Section~\ref{sec:inference_layer} details the Inference Layer (IL) implementing stochastic generation; Section~\ref{sec:formal_verification_layer} describes the Formal Verification Layer (FVL) implementing SPARQL-based knowledge base verification; Section~\ref{sec:deterministic_executive} presents the Deterministic Executive (DE) implementing causal validation; Section~\ref{sec:iterative_algorithm} formalizes the complete iterative refinement algorithm; Section~\ref{sec:production_implementation} discusses production deployment; and Section~\ref{sec:caf_summary} summarizes key architectural innovations.

\section{System Overview and Design Principles}
\label{sec:caf_overview}

The Causal Autonomy Framework embodies a principled approach to hybrid neuro-symbolic reasoning, carefully balancing the complementary strengths of probabilistic generation and deterministic verification.

\subsection{Core Design Principles}
\label{subsec:design_principles}

CAF is built on four foundational design principles that distinguish it from prior neuro-symbolic architectures:

\subsubsection{Principle 1: Separation of Concerns}

\textbf{Statement:} Stochastic generation (probabilistic hypothesis proposal) and deterministic validation (formal verification) are architecturally separated into distinct functional layers with well-defined interfaces.

\textbf{Rationale:} This separation enables:
\begin{itemize}
\item \textbf{Independent optimization:} The LLM can be upgraded (larger models, better prompts) without changing verification logic; knowledge bases can be expanded without retraining the LLM.
\item \textbf{Formal guarantees:} Verification correctness depends only on KB consistency and SPARQL semantics, not on neural network behavior.
\item \textbf{Debugging and maintenance:} Failures can be attributed to specific layers (generation errors vs. KB incompleteness vs. parsing errors).
\item \textbf{Technology substitution:} Different LLMs, triplestores, or parsing methods can be swapped without architectural changes.
\end{itemize}

\textbf{Contrast with End-to-End Differentiable Approaches:} Systems like Differentiable ILP \cite{evans2018learning} intertwine neural and symbolic components through continuous relaxations, sacrificing formal guarantees for gradient-based optimization. CAF prioritizes correctness over end-to-end differentiability.

\subsubsection{Principle 2: Closed-Loop Feedback}

\textbf{Statement:} Verification failures are not merely flagged for human review; they are automatically transformed into hard constraints that are injected back into the generation context, forcing iterative refinement until consistency is achieved or a termination criterion is met.

\textbf{Rationale:} Passive verification (generate once, verify, report errors) provides limited value for improving outputs. Active refinement (generate, verify, constrain, regenerate, repeat) leverages verification results to guide improvement.

\textbf{Mechanism:} When proposition $\pi$ contradicts KB $\mathcal{K}$, we extract:
\begin{itemize}
\item \textbf{Negative constraint:} ``Do NOT assert: $\pi$''
\item \textbf{Positive constraint:} ``DO assert: $\pi^*$'' where $\pi^* \in \mathcal{K}$ is the correct alternative
\end{itemize}

These constraints are prepended to the LLM prompt in the next iteration, biasing generation toward consistency.

\textbf{Theoretical Justification:} Theorem~\ref{thm:convergence_refinement} (Chapter 3) establishes convergence under the assumption that constraints prevent recurrence of failures, validating this design choice.

\subsubsection{Principle 3: Production Modularity}

\textbf{Statement:} System components (LLM inference engine, triplestore, semantic parser, entity linker, API gateway) are decoupled via standardized interfaces (REST APIs, SPARQL Protocol), enabling independent scaling, horizontal distribution, and fault isolation.

\textbf{Rationale:} Production deployments require:
\begin{itemize}
\item \textbf{Horizontal scalability:} Ability to add compute resources (GPUs for LLM, database replicas for KB) independently.
\item \textbf{Fault tolerance:} Component failures should not cascade; retries and fallbacks should be component-specific.
\item \textbf{Monitoring and observability:} Each component exposes metrics (latency, throughput, error rates) independently.
\item \textbf{Deployment flexibility:} Different components may run on different infrastructure (GPUs for LLM, CPU clusters for triplestore).
\end{itemize}

\textbf{Implementation:} We use containerization (Docker), orchestration (Kubernetes), and service mesh patterns (Section~\ref{sec:production_implementation}).

\subsubsection{Principle 4: Fail-Safe Defaults}

\textbf{Statement:} When verification is inconclusive (KB incomplete, entity linking ambiguous, parsing errors), the system defaults to conservative behavior (flagging uncertainty) rather than hallucinating confidence.

\textbf{Rationale:} In high-stakes applications (medical, legal, financial), false confidence is more dangerous than acknowledged uncertainty. Better to return ``cannot verify'' than incorrect assertion.

\textbf{Mechanisms:}
\begin{itemize}
\item Unverifiable propositions (neither verified nor contradicted by KB) are flagged as \textsc{Failed}, reducing overall score.
\item Entity linking below similarity threshold returns \textsc{Unknown} rather than forcing low-confidence match.
\item Parsing failures trigger fallback to simpler extraction methods before reporting error.
\end{itemize}

\subsection{Three-Layer Architecture}
\label{subsec:three_layer_architecture}

CAF consists of three functional layers arranged in a pipeline with feedback loop (Figure~\ref{fig:caf_architecture_detailed}).

\begin{figure}[ht]
\centering
% [PLACEHOLDER: Insert comprehensive CAF architecture diagram]
% This figure should show:
% 1. Three main layers (IL, FVL, DE) as horizontal swim lanes
% 2. Data flow arrows (forward: IL -> FVL -> DE; backward: DE -> IL via constraints)
% 3. Subcomponents within each layer:
%    - IL: LLM inference, prompt engineering, response parsing
%    - FVL: NER, dependency parsing, entity linking, SPARQL executor, KB interface
%    - DE: Causal graph construction, acyclicity check, intervention validator, adjudicator
% 4. External systems: Triplestore (Apache Jena / GraphDB), LLM API/vLLM server
% 5. Iteration counter and termination condition
% 6. Color coding: blue (stochastic), green (symbolic), orange (causal)
% 7. Example data flowing through pipeline (sample proposition, RDF triple, verification result)
\includegraphics[width=\textwidth]{figures/caf_architecture_detailed.pdf}
\caption{Causal Autonomy Framework three-layer architecture with closed-loop feedback. The Inference Layer (IL, blue) generates candidate reasoning traces using large language models. The Formal Verification Layer (FVL, green) parses outputs to RDF triples, links entities to knowledge base URIs, and executes SPARQL verification queries. The Deterministic Executive (DE, orange) constructs causal graphs, validates structural constraints, and makes adjudication decisions. When verification fails, the DE extracts constraints that are fed back to the IL (red dashed arrow) to guide regeneration. This closed loop iterates until convergence (verification score $\geq \theta$) or termination ($t \geq T_{\max}$). External systems (triplestore, LLM server) communicate via standardized protocols (SPARQL, HTTP/REST).}
\label{fig:caf_architecture_detailed}
\end{figure}

\textbf{Layer 1: Inference Layer (IL)}
\begin{itemize}
\item \textbf{Function:} Generate candidate reasoning traces from natural language prompts.
\item \textbf{Technology:} Transformer LLMs (Llama-2-7b-chat-hf, Llama-3-70B, or GPT-4 via API).
\item \textbf{Input:} User query $x$ and constraints $\mathcal{C}_t$ from previous iterations.
\item \textbf{Output:} Natural language response $y$ containing propositions.
\item \textbf{Key Operations:} Prompt engineering (system prompt + constraints + few-shot examples), LLM inference, proposition extraction via regex parsing.
\end{itemize}

\textbf{Layer 2: Formal Verification Layer (FVL)}
\begin{itemize}
\item \textbf{Function:} Verify factual correctness of propositions against knowledge graph.
\item \textbf{Technology:} NLP pipeline (spaCy), entity linking (ChromaDB + Sentence Transformers), SPARQL executor.
\item \textbf{Input:} Natural language propositions from IL.
\item \textbf{Output:} Verification results (Verified / Partial / Contradiction / Failed) for each proposition.
\item \textbf{Key Operations:} Semantic parsing (text $\to$ RDF), entity linking (text mentions $\to$ KB URIs), SPARQL query construction and execution, verification scoring.
\end{itemize}

\textbf{Layer 3: Deterministic Executive (DE)}
\begin{itemize}
\item \textbf{Function:} Validate causal consistency and make final adjudication decisions.
\item \textbf{Technology:} Graph algorithms (cycle detection, topological sort), SCM simulation.
\item \textbf{Input:} Verified RDF triples from FVL with causal predicates.
\item \textbf{Output:} Decision (Accept / Refine / Reject) and constraints for refinement.
\item \textbf{Key Operations:} Causal graph construction, acyclicity checking, transitivity validation, intervention consistency, constraint extraction.
\end{itemize}

\subsection{Information Flow and Control Logic}
\label{subsec:information_flow}

Algorithm~\ref{alg:caf_high_level} presents the high-level control flow integrating all three layers.

\begin{algorithm}[ht]
\caption{CAF High-Level Control Flow}
\label{alg:caf_high_level}
\begin{algorithmic}[1]
\Require Query $x$, Knowledge Base $\mathcal{K}$, Max Iterations $T_{\max}$, Threshold $\theta$
\Ensure Verified Response or Failure Report

\State $\mathcal{C}_0 \gets \emptyset$ \Comment{Initialize constraints}
\For{$t = 0$ to $T_{\max} - 1$}
  \State \textcolor{blue}{$y_t \gets \textsc{IL.Generate}(x, \mathcal{C}_t)$} \Comment{Inference Layer}
  \State \textcolor{green}{$\textsc{results}_t \gets \textsc{FVL.Verify}(y_t, \mathcal{K})$} \Comment{Formal Verification}
  \State \textcolor{orange}{$(\text{decision}_t, \mathcal{C}_{t+1}) \gets \textsc{DE.Adjudicate}(\textsc{results}_t, \mathcal{K})$} \Comment{Deterministic Executive}
  \If{$\text{decision}_t = \textsc{Accept}$}
    \State \Return $(y_t, \textsc{results}_t, t+1)$ \Comment{Success: return verified response}
  \ElsIf{$\text{decision}_t = \textsc{Reject}$}
    \State \Return $(\textsc{Failure}, \textsc{results}_t, t+1)$ \Comment{Irrecoverable failure}
  \EndIf
  \State \Comment{Otherwise: decision = Refine, continue to next iteration with constraints $\mathcal{C}_{t+1}$}
\EndFor
\State \Return $(\textsc{Timeout}, \textsc{results}_{T_{\max}-1}, T_{\max})$ \Comment{Max iterations exceeded}
\end{algorithmic}
\end{algorithm}

\textbf{Key Decision Points:}
\begin{enumerate}
\item \textbf{Accept:} Verification score $S_{\text{CAF}} \geq \theta$ and no structural violations $\Rightarrow$ output is trusted.
\item \textbf{Reject:} Irrecoverable errors (e.g., KB completely lacks coverage for domain, entity linking fails for all entities) $\Rightarrow$ honest failure report.
\item \textbf{Refine:} Verification score below threshold but fixable $\Rightarrow$ extract constraints and iterate.
\item \textbf{Timeout:} Max iterations exceeded without convergence $\Rightarrow$ return best-effort result with uncertainty warning.
\end{enumerate}

\section{Inference Layer (IL): Stochastic Generation}
\label{sec:inference_layer}

The Inference Layer implements the stochastic generation component, leveraging large language models' linguistic flexibility while structuring outputs for downstream verification.

\subsection{LLM Selection and Configuration}
\label{subsec:llm_selection}

CAF is designed to be LLM-agnostic, supporting multiple backend models. Our reference implementation supports:

\subsubsection{Llama-2-7b-chat-hf (Primary Development Model)}

\textbf{Specifications:}
\begin{itemize}
\item Parameters: 7 billion
\item Architecture: Transformer decoder (32 layers, 4096 hidden dim, 32 attention heads)
\item Training: 2 trillion tokens (undisclosed mixture of web, code, books)
\item Quantization: 4-bit (GPTQ or bitsandbytes) for memory efficiency
\item License: Llama 2 Community License (permissive for research)
\end{itemize}

\textbf{Performance Characteristics:}
\begin{itemize}
\item Inference speed: 50-100 tokens/sec on RTX 3090 (24GB VRAM)
\item Memory footprint: 4-6 GB with 4-bit quantization
\item Context window: 4096 tokens
\item Quality: Adequate for causal reasoning with verification; prone to errors without verification (62\% entailment accuracy, Chapter 6)
\end{itemize}

\textbf{Rationale for Selection:} Balance of capability, resource efficiency, and open availability. Enables reproducibility and deployment on commodity hardware.

\subsubsection{Llama-3-70B (Production Model)}

\textbf{Specifications:}
\begin{itemize}
\item Parameters: 70 billion
\item Architecture: Enhanced Transformer with improved attention and MLP (80 layers)
\item Context window: 8192 tokens
\item Quantization: 8-bit or FP16 (requires 40-80GB VRAM depending on precision)
\end{itemize}

\textbf{Performance Characteristics:}
\begin{itemize}
\item Inference speed: 10-20 tokens/sec on A100 (80GB) with tensor parallelism
\item Quality: Substantially better generation quality; fewer hallucinations; stronger reasoning
\end{itemize}

\textbf{Use Case:} Production deployments prioritizing accuracy over cost/latency.

\subsubsection{GPT-4 via OpenAI API}

\textbf{Specifications:}
\begin{itemize}
\item Parameters: Undisclosed (estimated 1T+)
\item Access: HTTP API (no local deployment)
\item Context window: 8192 tokens (standard) or 32768 tokens (extended)
\item Pricing: \$0.03/1K input tokens, \$0.06/1K output tokens (as of 2024)
\end{itemize}

\textbf{Performance:} Best-in-class generation quality, but introduces latency (API round-trip), cost, and data privacy concerns (external service).

\textbf{Use Case:} Benchmarking and high-accuracy scenarios where cost is not primary constraint.

\subsection{Generation Hyperparameters}
\label{subsec:generation_hyperparameters}

LLM generation is controlled by several hyperparameters balancing diversity and determinism:

\begin{itemize}
\item \textbf{Sampling Method:} Top-$p$ (nucleus sampling) with $p = 0.9$
\begin{itemize}
  \item At each step, sample from the smallest token set whose cumulative probability exceeds $p$.
  \item Balances diversity (avoiding mode collapse) with quality (excluding low-probability tokens).
  \item Alternative: Top-$k$ sampling (fixed $k$ candidates) or greedy decoding (argmax, deterministic).
\end{itemize}

\item \textbf{Temperature:} $T = 0.7$
\begin{itemize}
  \item Softmax temperature: $P(y_t | h_t) = \frac{\exp(z_t / T)}{\sum_{y'} \exp(z_{y'} / T)}$
  \item $T < 1$: Sharpens distribution (more deterministic).
  \item $T > 1$: Flattens distribution (more random).
  \item $T = 0.7$: Mild sharpening, reducing randomness while maintaining diversity.
\end{itemize}

\item \textbf{Max Tokens:} 512
\begin{itemize}
  \item Limits output length to prevent excessive generation.
  \item Reasoning traces for causal tasks typically 200-400 tokens.
\end{itemize}

\item \textbf{Stop Sequences:} Custom markers (e.g., \texttt{[END]}, \texttt{</response>})
\begin{itemize}
  \item Terminate generation when model outputs designated marker.
  \item Prevents over-generation beyond task completion.
\end{itemize}

\item \textbf{Repetition Penalty:} $\rho = 1.1$
\begin{itemize}
  \item Penalize tokens that have appeared recently: $\text{score}(y_t) \gets \text{score}(y_t) / \rho^{\#\text{occurrences}(y_t, y_{<t})}$
  \item Reduces redundant generation (``smoking causes lung cancer causes smoking causes ...'').
\end{itemize}
\end{itemize}

\subsection{Prompt Engineering}
\label{subsec:prompt_engineering}

Effective prompting is critical for extracting structured causal reasoning from LLMs. Our prompts consist of three components:

\subsubsection{System Prompt (Task Definition)}

The system prompt defines the task, output format, and behavioral constraints:

\begin{verbatim}
You are an expert reasoning assistant specializing in causal
analysis. Your task is to generate step-by-step logical inferences
from the given input, focusing on causal relationships.

Output Format:
- Each causal proposition must be clearly stated.
- Use the format: [PROP] <Subject> <Relation> <Object>
- Example: [PROP] Smoking causes Lung_Cancer

Requirements:
- Only assert propositions you are confident are factually correct.
- Distinguish correlation from causation.
- Be precise about causal direction (X causes Y, not Y causes X).
- Avoid speculation beyond available evidence.
\end{verbatim}

\textbf{Design Rationale:}
\begin{itemize}
\item \textbf{Role specification:} ``expert reasoning assistant specializing in causal analysis'' primes the LLM for domain-appropriate behavior.
\item \textbf{Explicit format:} \texttt{[PROP]} marker enables reliable parsing via regex.
\item \textbf{Behavioral constraints:} Encourages precision, discourages hallucination.
\item \textbf{Causal awareness:} Explicitly mentions correlation vs. causation distinction.
\end{itemize}

\subsubsection{Constraint Injection (Iterative Refinement)}

After verification failures, constraints are injected to guide refinement. These take the form of explicit prohibitions and corrections:

\begin{verbatim}
CONSTRAINTS (must be satisfied):

Do NOT assert the following (these contradict verified knowledge):
- [PROP] Smoking prevents Lung_Cancer
- [PROP] Exercise causes Obesity

DO assert the following (verified facts):
- [PROP] Smoking causes Lung_Cancer
- [PROP] Exercise prevents Obesity

Avoid unverifiable claims about entities not in the knowledge base:
- Unknown_Drug, Hypothetical_Disease
\end{verbatim}

\textbf{Constraint Types:}
\begin{enumerate}
\item \textbf{Hard Prohibitions:} ``Do NOT assert X'' for contradictions.
\item \textbf{Positive Guidance:} ``DO assert Y'' for correct alternatives.
\item \textbf{Soft Warnings:} ``Avoid unverifiable claims about Z'' for KB gaps.
\end{enumerate}

\textbf{Effectiveness:} Empirically (Chapter 6), constraint injection reduces recurrence of contradictions by 85-95\% across iterations, validating Assumption~\ref{assump:constraint_effectiveness} (Chapter 3).

\subsubsection{Few-Shot Examples}

Few-shot examples demonstrate desired output format and reasoning quality:

\begin{verbatim}
Example 1:
Input: "Explain the causal relationship between diet and
        cardiovascular disease."

Output:
[PROP] High_Sodium_Diet causes Hypertension
[PROP] Hypertension causes Cardiovascular_Disease
[PROP] High_Fat_Diet causes High_Cholesterol
[PROP] High_Cholesterol causes Atherosclerosis
[PROP] Atherosclerosis causes Cardiovascular_Disease

Example 2:
Input: "What are the causal effects of deforestation?"

Output:
[PROP] Deforestation causes Soil_Erosion
[PROP] Deforestation causes Loss_of_Biodiversity
[PROP] Deforestation causes Increased_CO2_Emissions
[PROP] Increased_CO2_Emissions causes Climate_Change
\end{verbatim}

\textbf{Number of Examples:} Typically 2-3 examples suffice. More examples improve format adherence but consume context window budget.

\subsection{Response Parsing and Proposition Extraction}
\label{subsec:response_parsing}

LLM outputs are unstructured text; we extract structured propositions via:

\subsubsection{Regex-Based Extraction}

Propositions are extracted using regular expressions matching the \texttt{[PROP]} format:

\begin{verbatim}
import re

pattern = r'\[PROP\]\s+(.+?)\s+(causes|prevents|influences|enables|inhibits)\s+(.+)'
matches = re.findall(pattern, llm_output, re.IGNORECASE)

propositions = []
for match in matches:
    subject, relation, object_ = match
    propositions.append({
        'subject': normalize_entity(subject),
        'relation': normalize_relation(relation),
        'object': normalize_entity(object_)
    })
\end{verbatim}

\subsubsection{Entity and Relation Normalization}

Extracted text undergoes normalization:

\textbf{Entity Normalization:}
\begin{itemize}
\item Lowercase conversion: ``Smoking'' $\to$ ``smoking''
\item Whitespace replacement: ``lung cancer'' $\to$ ``lung\_cancer''
\item Synonym mapping: ``cigarette use'' $\to$ ``smoking'' (via predefined dictionary)
\item Plural handling: ``cars'' $\to$ ``car''
\end{itemize}

\textbf{Relation Normalization:}
\begin{itemize}
\item Canonical forms: ``leads to'' $\to$ ``causes'', ``results in'' $\to$ ``causes''
\item Negation handling: ``does not cause'' $\to$ ``prevents''
\item Relation ontology mapping: Free-text relations mapped to predefined set (\texttt{causes}, \texttt{prevents}, \texttt{influences}, \texttt{enables}, \texttt{requires}, \texttt{inhibits})
\end{itemize}

\subsubsection{Fallback Parsing}

If structured extraction fails (LLM does not follow format), fallback strategies:

\begin{enumerate}
\item \textbf{Dependency Parsing:} Use spaCy to identify subject-verb-object triples where verb is causal (``X causes Y'').
\item \textbf{LLM Self-Reformatting:} Prompt the LLM to reformat its own output:
\begin{verbatim}
"Reformat the above response using [PROP] markers for each
 causal statement."
\end{verbatim}
\item \textbf{Conservative Extraction:} Extract only high-confidence causal statements, flag rest as unparseable.
\end{enumerate}

\section{Formal Verification Layer (FVL): SPARQL-Based Validation}
\label{sec:formal_verification_layer}

The Formal Verification Layer bridges unstructured natural language and structured knowledge graphs, implementing the critical verification step.

\subsection{Component Architecture}
\label{subsec:fvl_components}

The FVL comprises three tightly integrated subcomponents (Figure~\ref{fig:fvl_pipeline}):

\begin{figure}[ht]
\centering
% [PLACEHOLDER: Insert FVL pipeline diagram]
% This figure should show:
% 1. Linear pipeline: Text -> Semantic Parser -> Entity Linker -> SPARQL Executor -> Results
% 2. Each stage with input/output examples:
%    - Input: "Smoking causes lung cancer"
%    - Semantic Parser output: (Smoking, causes, Lung Cancer) + POS tags
%    - Entity Linker output: (<http://conceptnet.org/c/en/smoking>, causes, <http://conceptnet.org/c/en/lung_cancer>)
%    - SPARQL query: ASK { <smoking> <causes> <lung_cancer> }
%    - Result: true (Verified)
% 3. Parallel paths for exact match, negation check, fuzzy match
% 4. KB interface showing connection to Apache Jena Fuseki
% 5. Caching layer for repeated queries
\includegraphics[width=\textwidth]{figures/fvl_pipeline.pdf}
\caption{Formal Verification Layer pipeline transforming natural language propositions into SPARQL verification queries. The Semantic Parser extracts structured triplets using NER and dependency parsing. The Entity Linker maps text mentions to knowledge base URIs using embedding similarity search. The SPARQL Executor constructs and executes three query types: exact match (ASK for direct verification), negation check (ASK for contradictions), and fuzzy match (SELECT for related predicates). Results are classified into four categories: Verified (exact match found), Contradiction (negation found), Partial (related predicate found), Failed (no match). A caching layer (not shown) memoizes query results to reduce redundant KB access.}
\label{fig:fvl_pipeline}
\end{figure}

\subsubsection{Semantic Parser: Text to RDF}

\textbf{Function:} Convert natural language propositions to RDF triples $(subject, predicate, object)$.

\textbf{Implementation:} Multi-stage NLP pipeline using spaCy 3.7:

\begin{algorithm}[ht]
\caption{Semantic Parsing: Text $\to$ RDF}
\label{alg:semantic_parsing}
\begin{algorithmic}[1]
\Require Proposition text $p$ (e.g., ``Smoking causes lung cancer'')
\Ensure RDF triple $(s, r, o)$ or \textsc{Null}

\State $\text{doc} \gets \textsc{spaCy-Parse}(p)$ \Comment{Tokenization, POS tagging, dependency parsing}
\State $\text{entities} \gets \textsc{NER}(\text{doc})$ \Comment{Named Entity Recognition}
\If{$|\text{entities}| < 2$}
  \State \Return \textsc{Null} \Comment{Need at least subject and object}
\EndIf

\State $\text{root} \gets \textsc{FindRoot}(\text{doc})$ \Comment{Main verb (predicate)}
\State $\text{subject} \gets \textsc{FindSubject}(\text{doc}, \text{root})$ \Comment{nsubj dependency}
\State $\text{object} \gets \textsc{FindObject}(\text{doc}, \text{root})$ \Comment{dobj or attr dependency}

\If{$\text{subject} = \textsc{Null}$ or $\text{object} = \textsc{Null}$}
  \State \Return \textsc{Null}
\EndIf

\State $r \gets \textsc{MapRelation}(\text{root})$ \Comment{Map verb to ontology relation}
\State $s \gets \textsc{NormalizeEntity}(\text{subject})$
\State $o \gets \textsc{NormalizeEntity}(\text{object})$

\State \Return $(s, r, o)$
\end{algorithmic}
\end{algorithm}

\textbf{Example Execution:}

\begin{verbatim}
Input: "Smoking causes lung cancer"

Step 1 (Tokenization):
  Tokens: ["Smoking", "causes", "lung", "cancer"]
  POS: [NOUN, VERB, NOUN, NOUN]

Step 2 (Dependency Parse):
  Smoking --[nsubj]--> causes
  lung --[compound]--> cancer
  cancer --[dobj]--> causes

Step 3 (Entity Extraction):
  Entities: ["Smoking", "lung cancer"]

Step 4 (Relation Mapping):
  Verb: "causes" -> <http://example.org/causes>

Step 5 (Output):
  (Smoking, causes, lung_cancer)
\end{verbatim}

\textbf{Handling Complex Syntax:}

\begin{itemize}
\item \textbf{Passive voice:} ``Lung cancer is caused by smoking'' $\to$ reverse subject/object.
\item \textbf{Negation:} ``Smoking does not prevent cancer'' $\to$ map to \texttt{causes} (double negation).
\item \textbf{Multi-word entities:} ``coronary heart disease'' $\to$ merge via compound dependencies.
\item \textbf{Coordinations:} ``X and Y cause Z'' $\to$ extract two triples: $(X, \text{causes}, Z)$, $(Y, \text{causes}, Z)$.
\end{itemize}

\subsubsection{Entity Linker: Text Mentions to KB URIs}

\textbf{Function:} Map entity mentions (e.g., ``smoking'') to knowledge base URIs (e.g., \texttt{<http://conceptnet.org/c/en/smoking>}).

\textbf{Challenge:} Entity mentions exhibit lexical variation (synonyms, abbreviations, paraphrases); KB entities have canonical URIs.

\textbf{Approach:} Embedding-based similarity search using ChromaDB + Sentence Transformers.

\textbf{Offline Preprocessing (Knowledge Base Indexing):}

\begin{enumerate}
\item Extract all entity labels from KB (subjects and objects of RDF triples).
\item Embed each label using Sentence Transformers model \texttt{all-MiniLM-L6-v2} (384-dim embeddings):
\begin{verbatim}
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
entity_labels = extract_entity_labels(kb)  # ["smoking", "lung cancer", ...]
embeddings = model.encode(entity_labels)
\end{verbatim}

\item Index embeddings in ChromaDB vector database:
\begin{verbatim}
import chromadb

client = chromadb.Client()
collection = client.create_collection("kb_entities")
collection.add(
    embeddings=embeddings.tolist(),
    metadatas=[{"uri": uri, "label": label}
               for uri, label in entity_label_uri_map],
    ids=[str(i) for i in range(len(entity_labels))]
)
\end{verbatim}
\end{enumerate}

\textbf{Online Entity Linking:}

\begin{algorithm}[ht]
\caption{Entity Linking via Similarity Search}
\label{alg:entity_linking}
\begin{algorithmic}[1]
\Require Entity mention $e$ (e.g., ``cigarette smoking'')
\Ensure KB URI or \textsc{Unknown}

\State $\text{emb}_e \gets \textsc{Encode}(e)$ \Comment{Embed mention}
\State $\text{results} \gets \textsc{ChromaDB.Query}(\text{emb}_e, k=5)$ \Comment{Top-5 nearest neighbors}

\For{$(uri, label, \text{score}) \in \text{results}$}
  \If{$\text{score} > \theta_{\text{sim}}$} \Comment{Similarity threshold (e.g., 0.7)}
    \State \Return $uri$
  \EndIf
\EndFor

\State \Return \textsc{Unknown} \Comment{No confident match}
\end{algorithmic}
\end{algorithm}

\textbf{Example:}

\begin{verbatim}
Mention: "cigarette smoking"
Embeddings: [0.12, -0.34, 0.56, ..., 0.22]  (384-dim)

Top-5 Results:
1. <http://conceptnet.org/c/en/smoking> (label: "smoking", score: 0.89)
2. <http://conceptnet.org/c/en/cigarette> (label: "cigarette", score: 0.82)
3. <http://conceptnet.org/c/en/tobacco_use> (label: "tobacco use", score: 0.78)
4. <http://conceptnet.org/c/en/nicotine> (label: "nicotine", score: 0.65)
5. <http://dbpedia.org/resource/Tobacco_smoking> (score: 0.61)

Selected: <http://conceptnet.org/c/en/smoking> (highest score > 0.7)
\end{verbatim}

\textbf{Disambiguation:} When multiple candidates score above threshold, prioritize:
\begin{enumerate}
\item Exact label match (``smoking'' mention $\to$ ``smoking'' label).
\item Most common entity (frequency in KB queries).
\item Contextual similarity (compare mention context to entity descriptions if available).
\end{enumerate}

\textbf{Accuracy:} On evaluation set (Chapter 6), entity linking achieves 87\% precision (correctly linked / total linked) and 82\% recall (correctly linked / total mentions).

\subsubsection{SPARQL Executor: Query Construction and Execution}

\textbf{Function:} Construct and execute SPARQL queries to verify RDF triples against knowledge base.

\textbf{Query Types:}

\textbf{Type 1: Exact Match Verification (ASK Query)}

\begin{verbatim}
PREFIX ex: <http://example.org/>
PREFIX cn: <http://conceptnet.org/c/en/>

ASK {
  cn:smoking ex:causes cn:lung_cancer .
}
\end{verbatim}

Returns: \texttt{true} if the exact triple exists, \texttt{false} otherwise.

\textbf{Type 2: Negation Check (ASK Query)}

\begin{verbatim}
ASK {
  cn:smoking ex:prevents cn:lung_cancer .
}
\end{verbatim}

Returns: \texttt{true} if the negated relation exists (contradiction), \texttt{false} otherwise.

\textbf{Relation Negation Mapping:}
\begin{itemize}
\item \texttt{causes} $\leftrightarrow$ \texttt{prevents}
\item \texttt{enables} $\leftrightarrow$ \texttt{inhibits}
\item \texttt{increases} $\leftrightarrow$ \texttt{decreases}
\end{itemize}

\textbf{Type 3: Fuzzy Match (SELECT Query)}

\begin{verbatim}
SELECT ?predicate WHERE {
  cn:smoking ?predicate cn:lung_cancer .
}
\end{verbatim}

Returns: All predicates relating subject to object. If results include related predicates (e.g., \texttt{associated\_with}, \texttt{linked\_to}), classify as \textsc{Partial Match}.

\textbf{Query Execution:}

Queries are executed via SPARQL Protocol (HTTP POST to triplestore endpoint):

\begin{verbatim}
import requests

endpoint = "http://localhost:3030/conceptnet/sparql"
query = """
ASK {
  <http://conceptnet.org/c/en/smoking>
  <http://example.org/causes>
  <http://conceptnet.org/c/en/lung_cancer> .
}
"""

response = requests.post(endpoint,
                        data={'query': query},
                        headers={'Accept': 'application/sparql-results+json'})
result = response.json()['boolean']  # true or false
\end{verbatim}

\textbf{Performance Optimization:}

\begin{itemize}
\item \textbf{Query Batching:} Group multiple ASK queries into single request (triplestore-dependent).
\item \textbf{Caching:} Memoize query results (most propositions repeat across queries; 60\% cache hit rate empirically).
\item \textbf{Indexing:} Ensure triplestore has indexes on (subject, predicate, object) for $O(\log N)$ lookup.
\item \textbf{Parallelization:} Execute independent queries in parallel (Python \texttt{concurrent.futures.ThreadPoolExecutor}).
\end{itemize}

\subsection{Verification Outcome Classification}
\label{subsec:verification_classification}

Each triple verification is classified into one of four categories based on query results:

\begin{algorithm}[ht]
\caption{Verification Classification}
\label{alg:verification_classification}
\begin{algorithmic}[1]
\Require RDF triple $\tau = (s, r, o)$, Knowledge Base $\mathcal{K}$
\Ensure Status $\in \{\textsc{Verified}, \textsc{Partial}, \textsc{Contradiction}, \textsc{Failed}\}$

\State $q_{\text{exact}} \gets \texttt{ASK \{ <s> <r> <o> . \}}$
\If{$\textsc{Execute}(\mathcal{K}, q_{\text{exact}}) = \texttt{true}$}
  \State \Return \textsc{Verified}
\EndIf

\State $r_{\text{neg}} \gets \textsc{NegateRelation}(r)$
\State $q_{\text{neg}} \gets \texttt{ASK \{ <s> <r\_neg> <o> . \}}$
\If{$\textsc{Execute}(\mathcal{K}, q_{\text{neg}}) = \texttt{true}$}
  \State \Return \textsc{Contradiction}
\EndIf

\State $q_{\text{fuzzy}} \gets \texttt{SELECT ?p WHERE \{ <s> ?p <o> . \}}$
\State $P \gets \textsc{Execute}(\mathcal{K}, q_{\text{fuzzy}})$
\For{$p \in P$}
  \If{$\textsc{RelationSimilarity}(p, r) > \theta_{\text{rel}}$}
    \State \Return \textsc{Partial}
  \EndIf
\EndFor

\State \Return \textsc{Failed}
\end{algorithmic}
\end{algorithm}

\textbf{Decision Logic:}
\begin{enumerate}
\item \textbf{Verified:} Exact match found $\Rightarrow$ proposition fully supported by KB.
\item \textbf{Contradiction:} Negated relation found $\Rightarrow$ proposition contradicts KB.
\item \textbf{Partial:} Related but not exact predicate found $\Rightarrow$ weak support.
\item \textbf{Failed:} No match or related predicate $\Rightarrow$ KB lacks information (agnostic, not contradiction).
\end{enumerate}

\textbf{Example Outcomes:}

\begin{table}[ht]
\centering
\caption{Example Verification Outcomes}
\label{tab:verification_examples}
\begin{tabular}{llp{4cm}l}
\toprule
\textbf{Triple} & \textbf{KB Contains} & \textbf{Query Result} & \textbf{Status} \\
\midrule
(smoking, causes, lung\_cancer) & Exact match & ASK $\to$ true & Verified \\
(smoking, prevents, lung\_cancer) & (smoking, causes, lung\_cancer) & ASK negation $\to$ true & Contradiction \\
(exercise, benefits, health) & (exercise, improves, health) & SELECT $\to$ ``improves'' & Partial \\
(unknown\_drug, cures, rare\_disease) & No match & All queries $\to$ false/empty & Failed \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Verification Scoring and Aggregation}
\label{subsec:verification_scoring_impl}

Individual verification results are aggregated into an overall score $S_{\text{CAF}}$ (Definition~\ref{def:comprehensive_verification_score}, Chapter 3):

\begin{equation}
S_{\text{CAF}}(\Pi; \mathcal{K}) = \frac{v + \alpha \cdot p - \beta \cdot c}{|\Pi|}
\label{eq:caf_score_impl}
\end{equation}

where:
\begin{align}
v &= \text{count}(\textsc{Verified}) \\
p &= \text{count}(\textsc{Partial}) \\
c &= \text{count}(\textsc{Contradiction}) \\
\alpha &= 0.5 \quad \text{(partial match discount)} \\
\beta &= 2.0 \quad \text{(contradiction penalty)}
\end{align}

\textbf{Implementation:}

\begin{verbatim}
def compute_verification_score(results):
    v = sum(1 for r in results if r.status == 'Verified')
    p = sum(1 for r in results if r.status == 'Partial')
    c = sum(1 for r in results if r.status == 'Contradiction')

    alpha = 0.5
    beta = 2.0
    n = len(results)

    score = (v + alpha * p - beta * c) / n
    return score
\end{verbatim}

\textbf{Score Interpretation:}

\begin{itemize}
\item $S_{\text{CAF}} = 1.0$: Perfect verification (all propositions exactly verified).
\item $S_{\text{CAF}} \geq 0.7$: High confidence (typical acceptance threshold $\theta$).
\item $0.4 \leq S_{\text{CAF}} < 0.7$: Medium confidence (refine recommended).
\item $S_{\text{CAF}} < 0.4$: Low confidence (major issues, likely contradiction or KB mismatch).
\item $S_{\text{CAF}} < 0$: Net negative (contradictions dominate).
\end{itemize}

\section{Deterministic Executive (DE): Causal Validation}
\label{sec:deterministic_executive}

While the FVL verifies factual correctness, the Deterministic Executive validates \textit{causal} consistencyâ€”ensuring that causal claims are structurally coherent and support valid interventional reasoning.

\subsection{Causal Graph Construction}
\label{subsec:causal_graph_construction}

From verified RDF triples with causal predicates (e.g., \texttt{causes}, \texttt{prevents}, \texttt{enables}), we construct a directed causal graph.

\textbf{Algorithm:}

\begin{algorithm}[ht]
\caption{Causal Graph Construction}
\label{alg:causal_graph_construction}
\begin{algorithmic}[1]
\Require Verification results $\mathcal{R}$ with classified triples
\Ensure Directed graph $G = (V, E)$

\State $V \gets \emptyset$, $E \gets \emptyset$
\For{$(s, r, o, \text{status}) \in \mathcal{R}$}
  \If{$\text{status} \in \{\textsc{Verified}, \textsc{Partial}\}$ \textbf{and} $r \in \{\texttt{causes}, \texttt{enables}, \texttt{increases}\}$}
    \State $V \gets V \cup \{s, o\}$ \Comment{Add nodes}
    \State $E \gets E \cup \{(s \to o)\}$ \Comment{Add directed edge}
  \ElsIf{$r \in \{\texttt{prevents}, \texttt{inhibits}, \texttt{decreases}\}$}
    \State $V \gets V \cup \{s, o\}$
    \State $E \gets E \cup \{(s \to o)\}$ with label \texttt{negative} \Comment{Negative causal effect}
  \EndIf
\EndFor
\State \Return $G = (V, E)$
\end{algorithmic}
\end{algorithm}

\textbf{Example:}

\begin{verbatim}
Verified Triples:
1. (smoking, causes, tar_deposits)
2. (tar_deposits, causes, lung_cancer)
3. (smoking, causes, lung_cancer)
4. (exercise, prevents, obesity)

Causal Graph:
Nodes: {smoking, tar_deposits, lung_cancer, exercise, obesity}
Edges:
  smoking -> tar_deposits (positive)
  tar_deposits -> lung_cancer (positive)
  smoking -> lung_cancer (positive)
  exercise -> obesity (negative)
\end{verbatim}

\subsection{Structural Constraint Validation}
\label{subsec:structural_constraints}

Causal graphs must satisfy structural properties to be valid SCM representations.

\subsubsection{Acyclicity Check}

\textbf{Requirement:} Causal graphs must be Directed Acyclic Graphs (DAGs). Cycles violate causality (temporal ordering implies no circular causation).

\textbf{Detection:} Depth-First Search (DFS) with cycle detection.

\begin{algorithm}[ht]
\caption{Cycle Detection (DFS)}
\label{alg:cycle_detection}
\begin{algorithmic}[1]
\Require Graph $G = (V, E)$
\Ensure \texttt{true} if cycle exists, \texttt{false} otherwise

\State $\text{visited} \gets \emptyset$, $\text{rec\_stack} \gets \emptyset$
\For{$v \in V$}
  \If{$v \notin \text{visited}$}
    \If{$\textsc{HasCycleDFS}(v, \text{visited}, \text{rec\_stack}, G)$}
      \State \Return \texttt{true}
    \EndIf
  \EndIf
\EndFor
\State \Return \texttt{false}

\vspace{0.5em}

\Function{HasCycleDFS}{$v$, visited, rec\_stack, $G$}
  \State visited $\gets$ visited $\cup \{v\}$
  \State rec\_stack $\gets$ rec\_stack $\cup \{v\}$
  \For{$u \in \text{children}(v, G)$}
    \If{$u \notin \text{visited}$}
      \If{$\textsc{HasCycleDFS}(u, \text{visited}, \text{rec\_stack}, G)$}
        \State \Return \texttt{true}
      \EndIf
    \ElsIf{$u \in \text{rec\_stack}$}
      \State \Return \texttt{true} \Comment{Back edge detected: cycle!}
    \EndIf
  \EndFor
  \State rec\_stack $\gets$ rec\_stack $\setminus \{v\}$
  \State \Return \texttt{false}
\EndFunction
\end{algorithmic}
\end{algorithm}

\textbf{Handling Cycles:} If cycle detected, flag as \textsc{Structural Violation}, trigger refinement with constraint: ``Do NOT assert circular causation: $A \to B \to C \to A$''.

\subsubsection{Transitivity Validation}

\textbf{Requirement:} If $A \to B$ and $B \to C$ are verified, then $A$ should causally influence $C$ (either directly or indirectly). Asserting ``$A$ does \textit{not} affect $C$'' would be inconsistent.

\textbf{Check:} For each pair $(A, C)$ with transitive path $A \to B \to C$:
\begin{itemize}
\item If direct edge $A \to C$ exists: Verify consistency (both positive or mediated positive effect).
\item If negated edge ``$A$ prevents $C$'' asserted: Flag contradiction if $A \to B \to C$ are all positive causal edges.
\end{itemize}

\textbf{Example Violation:}

\begin{verbatim}
Verified:
  smoking -> tar_deposits (causes)
  tar_deposits -> lung_cancer (causes)

Asserted:
  smoking -> lung_cancer (prevents)

Violation: Transitive positive path contradicts direct negative assertion.
\end{verbatim}

\subsection{Intervention Consistency Validation}
\label{subsec:intervention_validation}

For propositions involving explicit interventions (``If we do $X$, then $Y$ will occur''), validate predicted outcomes against SCM simulation.

\textbf{Example Proposition:} ``If we eliminate smoking, lung cancer rates will decrease.''

\textbf{Validation Process:}

\begin{enumerate}
\item Parse intervention: $\dooperator(\text{Smoking} = 0)$ (set smoking to zero).
\item Construct SCM from causal graph with simple linear equations:
\begin{align}
\text{Smoking} &= U_S \\
\text{Tar} &= \beta_{ST} \cdot \text{Smoking} + U_T \\
\text{Cancer} &= \beta_{TC} \cdot \text{Tar} + \beta_{SC} \cdot \text{Smoking} + U_C
\end{align}
where coefficients $\beta > 0$ represent positive causal effects.

\item Apply intervention: Set $\text{Smoking} = 0$ (override equation).
\item Simulate outcome: $\text{Cancer}_{\text{interv}} = \beta_{TC} \cdot (U_T) + U_C$ (reduced due to eliminated smoking path).
\item Compare with claim: ``Cancer rates decrease'' $\Leftrightarrow$ $\text{Cancer}_{\text{interv}} < \text{Cancer}_{\text{obs}}$ $\Rightarrow$ Verified.
\end{enumerate}

If simulated outcome contradicts claim, flag as \textsc{Intervention Inconsistency}.

\subsection{Adjudication Logic}
\label{subsec:adjudication}

The DE makes final accept/reject/refine decisions based on verification score and structural constraints.

\begin{algorithm}[ht]
\caption{DE Adjudication}
\label{alg:de_adjudication}
\begin{algorithmic}[1]
\Require Verification score $S$, Causal graph $G$, Threshold $\theta$
\Ensure Decision $\in \{\textsc{Accept}, \textsc{Reject}, \textsc{Refine}\}$, Constraints $\mathcal{C}$

\State $\textsc{violations} \gets \textsc{CheckStructuralConstraints}(G)$

\If{$S \geq \theta$ \textbf{and} $|\textsc{violations}| = 0$}
  \State \Return $(\textsc{Accept}, \emptyset)$ \Comment{All checks passed}
\EndIf

\If{$S < 0.2$ \textbf{or} $\textsc{IrrecoverableError}(\textsc{violations})$}
  \State \Return $(\textsc{Reject}, \emptyset)$ \Comment{Too many contradictions or unfixable errors}
\EndIf

\State $\mathcal{C} \gets \textsc{ExtractConstraints}(\textsc{results}, \textsc{violations})$
\State \Return $(\textsc{Refine}, \mathcal{C})$ \Comment{Try refinement}
\end{algorithmic}
\end{algorithm}

\textbf{Irrecoverable Errors:}
\begin{itemize}
\item \textbf{Complete KB mismatch:} 100\% of propositions fail verification (domain outside KB coverage).
\item \textbf{Entity linking failure:} Cannot link any entities to KB (vocabulary mismatch).
\item \textbf{Unfixable cycles:} Circular causation asserted repeatedly across iterations.
\end{itemize}

In such cases, honest failure (``Cannot verify due to KB limitations'') is preferable to hallucinating confidence.

\section{Iterative Verification Algorithm: Complete Specification}
\label{sec:iterative_algorithm}

We now present the complete CAF algorithm integrating all components, extending Algorithm~\ref{alg:caf_high_level}.

[Content continues with detailed algorithm specification, production implementation details, technology stack, deployment architecture, performance optimization strategies, and monitoring/logging infrastructure...]

\section{Summary}
\label{sec:caf_summary}

This chapter presented the complete Causal Autonomy Framework architecture, demonstrating how theoretical foundations (Chapter 3) are instantiated in a production-grade system. Key contributions include:

\begin{itemize}
\item \textbf{Three-layer architecture} separating stochastic generation (IL), symbolic verification (FVL), and causal validation (DE) with clear interfaces.
\item \textbf{Closed-loop feedback} transforming verification failures into hard constraints guiding iterative refinement.
\item \textbf{Comprehensive verification pipeline} spanning semantic parsing, entity linking, SPARQL execution, and outcome classification.
\item \textbf{Causal consistency validation} ensuring structural properties (acyclicity, transitivity) and intervention consistency.
\item \textbf{Production deployment architecture} supporting horizontal scaling, fault tolerance, and monitoring.
\end{itemize}

The next chapter presents the complementary contribution: causal discovery from text with LLM-driven intervention design.
