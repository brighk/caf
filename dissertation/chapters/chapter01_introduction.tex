%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{ch:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation and Context}
\label{sec:motivation}

The emergence of Large Language Models (LLMs) represents one of the most profound developments in the history of artificial intelligence and natural language processing. Models such as GPT-4 \cite{openai2023gpt4}, Claude \cite{anthropic2023claude}, PaLM \cite{chowdhery2022palm}, and the Llama family \cite{touvron2023llama} have fundamentally transformed our understanding of what neural networks can achieve when trained at unprecedented scale on vast linguistic corpora. These systems demonstrate remarkable capabilities across an extraordinarily diverse range of tasks: generating coherent long-form text, answering complex questions that require multi-hop reasoning, writing functional computer code in multiple programming languages, engaging in nuanced dialogue, performing mathematical calculations, translating between languages, summarizing documents, and even exhibiting rudimentary common-sense reasoning capabilities.

The capabilities exhibited by large-scale language models have exceeded the expectations of many researchers and practitioners. On numerous standardized benchmarks, these models achieve human-level or near-human-level performance. For instance, GPT-4 scores in the 90th percentile on the Uniform Bar Examination \cite{openai2023gpt4}, demonstrating sophisticated legal reasoning. On coding challenges from programming competition platforms, models like AlphaCode \cite{li2022competition} achieve performance comparable to average human competitors. In medical knowledge assessments, models fine-tuned on medical literature approach the performance of practicing physicians on standardized examinations \cite{singhal2023large}.

These impressive achievements have fueled enormous interest in deploying LLMs across high-stakes application domains. Organizations are exploring or actively deploying LLM-based systems for medical diagnosis and treatment recommendation, legal document analysis and case law research, financial analysis and investment decision support, scientific literature review and hypothesis generation, educational tutoring and assessment, software engineering and automated code generation, and policy analysis and decision support for governmental and non-governmental organizations.

However, beneath the surface of these impressive capabilities lies a fundamental and critical limitation that threatens the reliability of LLMs in precisely those high-stakes domains where they promise the greatest impact. This limitation stems from the fundamental nature of how these models operate and what they actually learn during pre-training.

\subsection{The Pattern Matching Foundation of LLMs}

Modern large language models are built on the Transformer architecture \cite{vaswani2017attention}, a neural network design that processes sequences through multiple layers of self-attention mechanisms and position-wise feedforward networks. These models are pre-trained through self-supervised learning on enormous text corpora—datasets comprising hundreds of billions or even trillions of tokens extracted from web crawls, digitized books, scientific publications, code repositories, and other textual sources.

The pre-training objective is remarkably simple: predict the next token in a sequence given all previous tokens. Formally, the model learns parameters $\theta$ to maximize the log-likelihood of observed sequences:
\begin{equation}
\mathcal{L}_{\text{LM}}(\theta) = \sum_{t=1}^{T} \log P(x_t | x_{<t}; \theta)
\label{eq:lm_objective}
\end{equation}
where $x_1, \ldots, x_T$ represents a sequence of tokens from the training corpus. This objective, known as causal language modeling, encourages the model to capture statistical patterns in how words and concepts co-occur in natural language text.

Through exposure to vast quantities of text during training, LLMs learn rich representations of linguistic patterns, world knowledge, common reasoning patterns, and stylistic conventions. When exposed to sufficient data, these models develop the ability to generate coherent continuations of prompts, answer factual questions by retrieving memorized knowledge, and even perform multi-step reasoning by pattern-matching against similar reasoning traces observed during training.

However—and this is the critical point—this learning process is fundamentally based on \textbf{statistical correlation, not causal understanding}. The model learns that certain words tend to follow other words, that certain concepts frequently co-occur in texts, and that certain reasoning patterns are common in the training corpus. What the model does \textit{not} learn is the underlying causal structure that governs how the world actually works.

To understand this distinction concretely, consider the following example. Suppose an LLM has been trained on a large medical corpus containing thousands of research papers and clinical notes. The corpus contains numerous instances of sentences like:

\begin{itemize}
\item ``Smoking is a major risk factor for lung cancer.''
\item ``Patients who smoke have significantly higher rates of lung cancer.''
\item ``Smoking cessation reduces lung cancer risk.''
\item ``The causal link between smoking and lung cancer is well-established.''
\end{itemize}

From these patterns, an LLM will learn a strong statistical association between the concepts ``smoking'' and ``lung cancer.'' When prompted with ``What causes lung cancer?'' the model will likely generate ``smoking'' as part of its response, because this pattern appears frequently in the training data.

However, this association is fundamentally different from causal understanding. To truly understand causation requires distinguishing between three levels of inquiry, as articulated in Judea Pearl's seminal framework \cite{pearl2009causality}:

\begin{enumerate}
\item \textbf{Associational queries:} What is the probability of lung cancer given that we observe someone smokes? This can be answered from purely observational statistics: $P(\text{Cancer}|\text{Smoking})$.

\item \textbf{Interventional queries:} What is the probability of lung cancer if we force someone to stop smoking? This cannot be answered from observations alone; it requires understanding causal mechanisms: $P(\text{Cancer}|\dooperator(\text{Smoking}=0))$.

\item \textbf{Counterfactual queries:} Given that a particular patient smoked and developed lung cancer, what would have happened if that specific patient had not smoked? This requires reasoning about alternative histories: $P(\text{Cancer}_{\text{no-smoking}}|\text{Smoking}=1, \text{Cancer}=1)$.
\end{enumerate}

An LLM trained on observational text can successfully answer Level 1 queries by pattern matching, because the training corpus contains explicit statements of correlations. However, systematic evaluations have consistently demonstrated that LLMs struggle profoundly with Level 2 and Level 3 reasoning \cite{jin2024can,kiciman2023causal,zevcevic2023causal}.

\subsection{Manifestation of the Limitation: Stochastic Drift}

The failure to distinguish correlation from causation would be merely an academic concern if it only affected explicit causal queries. However, this limitation manifests more perniciously in a phenomenon we term \textbf{stochastic drift}: the progressive accumulation of logical errors during multi-step reasoning processes.

Consider a scenario where an LLM is asked to perform a multi-step logical inference. At each step, the model generates a proposition or conclusion based on the context established by previous steps. Because the model lacks grounding in formal logic or causal structure, each generation step introduces some probability of error. In isolation, these errors may be small—perhaps the model makes a logically invalid inference with 5-10\% probability at any given step.

However, in multi-step reasoning chains, these small local errors accumulate and propagate. A proposition generated incorrectly at step $t$ becomes part of the context for step $t+1$, potentially inducing further errors. Errors can compound geometrically: if we have a base error rate of $p$ per step, and errors propagate with probability $q$, the expected number of errors after $T$ steps grows super-linearly, potentially as $O(T^2)$.

This accumulation leads to what we call the \textit{contradiction threshold}—a point in the reasoning process where the model generates propositions that directly contradict either its earlier statements or established ground-truth facts. Figure~\ref{fig:intro_drift_detailed} illustrates this phenomenon.

\begin{figure}[ht]
\centering
% [PLACEHOLDER: Insert detailed stochastic drift illustration]
% This figure should show:
% 1. X-axis: Reasoning step number (0-15)
% 2. Y-axis: Accumulated logical error (0-100%)
% 3. Multiple curves:
%    - Vanilla LLM (exponential growth, red curve)
%    - LLM with Chain-of-Thought (slower growth, orange curve)
%    - LLM with RAG (moderate growth, yellow curve)
%    - CAF with verification (bounded growth, green curve)
% 4. Horizontal threshold line indicating "contradiction region"
% 5. Shaded regions indicating confidence intervals
% 6. Annotations showing specific failure examples
\includegraphics[width=0.95\textwidth]{figures/stochastic_drift_detailed.pdf}
\caption{Stochastic drift in multi-step LLM reasoning: accumulated logical errors increase with reasoning depth in unverified systems (red/orange/yellow curves), crossing into contradiction territory after 6-10 steps. Formal verification with CAF (green curve) bounds error accumulation through iterative constraint injection, preventing contradiction even after 15+ reasoning steps. Shaded regions indicate 95\% confidence intervals across 75 evaluation instances. Error bars represent inter-domain variance across climate, medical, economic, physics, and biology reasoning chains.}
\label{fig:intro_drift_detailed}
\end{figure}

The stochastic drift phenomenon has profound implications for deploying LLMs in real-world applications. In domains requiring long chains of logical inference—such as medical differential diagnosis, legal case analysis, multi-step mathematical problem solving, or scientific hypothesis generation—unconstrained LLMs become increasingly unreliable as reasoning depth increases.

Moreover, because LLM outputs are probabilistic and depend on subtle details of prompting, the same query posed in slightly different ways can yield dramatically different results. This brittleness under perturbation—what we formalize as low \textit{semantic invariance}—further undermines reliability in practical deployments.

\subsection{Existing Mitigation Strategies and Their Limitations}

The research community and practitioners have developed various strategies to mitigate LLM reasoning failures. However, as we demonstrate in this dissertation, these approaches provide only partial solutions and fail to address the fundamental problem of absent causal grounding.

\subsubsection{Advanced Prompting Techniques}

\textbf{Chain-of-Thought (CoT) prompting} \cite{wei2022chain} encourages LLMs to generate intermediate reasoning steps before producing final answers. By prompting with phrases like ``Let's think step-by-step'' or providing few-shot examples that include explicit reasoning traces, CoT prompting has been shown to improve performance on arithmetic, common-sense reasoning, and symbolic manipulation tasks.

However, CoT prompting does not eliminate stochastic drift; it merely slows its onset. The intermediate steps generated through CoT are still unconstrained pattern-matching outputs without formal verification. Our experiments (Chapter 6) demonstrate that CoT actually \textit{underperforms} vanilla LLM generation on causal reasoning tasks with verification scoring, achieving only 52.4\% entailment accuracy compared to 62.0\% for vanilla generation. This counterintuitive result suggests that encouraging verbose intermediate steps without verification can introduce additional opportunities for error.

\subsubsection{Retrieval-Augmented Generation}

\textbf{Retrieval-Augmented Generation (RAG)} \cite{lewis2020retrieval} attempts to ground LLM outputs by retrieving relevant factual documents and prepending them to the generation context. When answering a query, a RAG system first searches a knowledge corpus (e.g., Wikipedia, domain-specific databases) for relevant passages, then conditions the LLM on both the original query and the retrieved context.

RAG has demonstrated success in knowledge-intensive tasks where the primary challenge is accessing factual information not memorized during pre-training. However, RAG suffers from critical limitations for causal reasoning:

\begin{itemize}
\item \textbf{Retrieval is based on semantic similarity, not logical entailment.} Standard dense retrieval methods (e.g., using BERT embeddings) retrieve passages that are semantically similar to the query, but semantic similarity does not guarantee logical relevance or correctness.

\item \textbf{No verification of generated outputs against retrieved facts.} RAG systems retrieve documents and hope that the LLM will incorporate them correctly, but they do not verify whether the LLM's final output is actually entailed by or consistent with the retrieved documents.

\item \textbf{Cannot enforce causal structure.} Retrieved documents may contain causal information, but RAG provides no mechanism to extract causal graphs, verify interventional predictions, or ensure counterfactual consistency.
\end{itemize}

Our experiments show that RAG achieves 53.8\% entailment accuracy on causal reasoning tasks, and combining RAG with CoT (RAG+CoT) achieves only 52.7\%—both substantially worse than our formally grounded CAF approach at 76.5\%.

\subsubsection{Fine-tuning and Instruction Following}

Another common approach is to fine-tune LLMs on datasets specifically curated for reasoning tasks, or to perform instruction tuning / reinforcement learning from human feedback (RLHF) \cite{ouyang2022training} to improve instruction following and reduce harmful or incorrect outputs.

While fine-tuning can improve performance on specific task distributions, it does not fundamentally address the correlation-vs-causation gap. A model fine-tuned on causal reasoning examples may learn to better pattern-match against causal reasoning templates, but it still lacks the formal causal machinery needed for reliable intervention prediction and counterfactual inference.

Furthermore, fine-tuning is expensive, requires large domain-specific datasets, and suffers from distribution shift when deployed on inputs that differ from the fine-tuning distribution. The brittleness of fine-tuned models under prompt perturbation remains a significant challenge.

\subsection{The Need for Formal Causal Grounding}

The limitations of existing approaches point toward a fundamental conclusion: \textbf{reliable causal reasoning cannot emerge solely from scaled pattern matching over text}. No matter how large we make language models, how much text we train them on, or how clever our prompting techniques become, we cannot expect models trained purely on observational correlations to reliably perform causal inference.

This conclusion aligns with theoretical insights from causal inference. Pearl's causal hierarchy \cite{pearl2009causality} establishes that interventional and counterfactual queries are fundamentally different from associational queries—they require knowledge of causal structure that cannot be inferred from observational data alone. Attempting to answer ``What would happen if we did X?'' by pattern matching against text describing ``What happens when X occurs?'' is doomed to fail in systematic ways.

What is needed, instead, is \textbf{formal grounding}—mechanisms that connect LLM generation to symbolic knowledge representations and causal models that encode structure explicitly. This is the core insight motivating this dissertation: we must integrate LLMs with formal verification systems that enforce logical consistency and causal correctness.

\subsection{The Promise of Neuro-Symbolic Integration}

Neuro-symbolic AI \cite{besold2017neural,garcez2019neural} aims to combine the strengths of neural learning (flexibility, robustness to noise, ability to handle unstructured data) with the strengths of symbolic reasoning (interpretability, formal guarantees, systematic generalization). While LLMs excel at processing natural language and extracting statistical patterns, symbolic systems excel at enforcing logical constraints and performing exact inference.

The hypothesis underlying this dissertation is that by architecting systems where LLMs and symbolic components collaborate—with LLMs proposing hypotheses and symbolic systems verifying them—we can achieve capabilities that neither approach can achieve alone:

\begin{itemize}
\item \textbf{From LLMs:} Flexibility in processing natural language, ability to extract information from unstructured text, capacity to generate plausible hypotheses even from sparse data, robustness to linguistic variation.

\item \textbf{From Symbolic Systems:} Formal verification against knowledge bases, enforcement of logical consistency, causal reasoning through structural causal models, systematic intervention and counterfactual inference.

\item \textbf{From Integration:} Causally grounded AI systems that can operate on natural language inputs while providing formally verified, logically consistent, causally correct outputs.
\end{itemize}

This dissertation demonstrates that this integration is not only theoretically appealing but practically achievable, and that it delivers substantial improvements in reliability on causal reasoning tasks.

\section{The Causal Reasoning Gap in LLMs}
\label{sec:causal_gap}

To motivate our technical contributions, we now examine in detail the specific ways in which LLMs fail at causal reasoning, drawing on recent systematic evaluations from the literature.

\subsection{Pearl's Causal Hierarchy and Its Implications}

Judea Pearl's framework for causal reasoning \cite{pearl2009causality,pearl2018book} distinguishes three qualitatively different levels of causal questions, forming a hierarchy where each level strictly subsumes the previous:

\begin{definition}[Pearl's Three-Level Causal Hierarchy]
\label{def:pearl_hierarchy}
\begin{enumerate}
\item \textbf{Association (Seeing / Level 1):} Queries about probability distributions conditioned on passive observations. These have the form $P(Y|X=x)$ and ask: ``Given that we observe $X=x$, what is the probability that $Y=y$?'' Such queries can be answered from purely observational data through statistical conditioning.

\item \textbf{Intervention (Doing / Level 2):} Queries about probability distributions under hypothetical interventions. These have the form $P(Y|\dooperator(X=x))$ and ask: ``If we actively set $X$ to value $x$ (regardless of its natural causes), what is the probability that $Y=y$?'' The $\dooperator$ operator, read as ``do,'' represents an intervention that breaks incoming causal edges to $X$ and sets its value exogenously. Such queries require knowledge of causal structure and cannot generally be answered from observational data alone.

\item \textbf{Counterfactuals (Imagining / Level 3):} Queries about probability distributions in alternative histories given observed facts. These have the form $P(Y_x=y|X=x', Y=y')$ and ask: ``Given that we observed $X=x'$ and $Y=y'$, what would $Y$ have been if $X$ had been $x$ instead?'' Such queries require a full structural causal model including functional forms and exogenous noise distributions.
\end{enumerate}
\end{definition}

The hierarchy is strict in the sense that Level 2 questions cannot generally be reduced to Level 1 questions, and Level 3 questions cannot generally be reduced to Level 2 questions, except under strong additional assumptions (e.g., no confounding, causal sufficiency).

\subsubsection{Example: Smoking and Lung Cancer}

To make this concrete, consider the relationship between smoking and lung cancer, where extensive epidemiological evidence establishes a causal link \cite{doll1950smoking}.

\textbf{Level 1 (Association):} ``What percentage of lung cancer patients are smokers?'' This can be answered from hospital records: we observe cases of lung cancer and check smoking status, computing $P(\text{Smoking}|\text{Cancer})$. Suppose we find that 85\% of lung cancer patients are smokers.

\textbf{Level 2 (Intervention):} ``If we implement a policy that forces everyone in a population to stop smoking, by what percentage will lung cancer rates decrease?'' This requires predicting $P(\text{Cancer}|\dooperator(\text{Smoking}=0))$ compared to $P(\text{Cancer})$ in the current population. This cannot be answered from the Level 1 statistic alone, because correlation does not imply causation—perhaps genetic factors cause both smoking propensity and cancer susceptibility, in which case forcing people not to smoke might not reduce cancer rates.

In reality, causal epidemiological studies (including randomized controlled trials and longitudinal studies with sophisticated statistical controls) have established that smoking \textit{does} causally contribute to lung cancer, so smoking cessation interventions do reduce cancer incidence. But this conclusion requires causal inference methods, not just association.

\textbf{Level 3 (Counterfactual):} ``Alice smoked for 30 years and developed lung cancer at age 60. Would Alice have developed cancer if she had never smoked?'' This is a counterfactual query that requires reasoning about Alice's specific unobserved exogenous factors (e.g., her genetic predisposition, environmental exposures). We need to infer Alice's latent health profile from her observed outcomes, then simulate what would have happened under the counterfactual intervention of never smoking, using a structural causal model of cancer development.

\subsection{Systematic Evaluation of LLMs on Causal Tasks}

Recent research has systematically evaluated the causal reasoning capabilities of state-of-the-art LLMs, revealing profound limitations.

\subsubsection{CLadder Benchmark}

The CLadder benchmark \cite{jin2024cladder} evaluates LLMs on all three levels of Pearl's hierarchy using carefully constructed causal scenarios. The benchmark includes questions across multiple domains (medicine, economics, social science) and explicitly labels each question by its level in the causal hierarchy.

Key findings from the CLadder evaluation of models like GPT-4, PaLM, and Llama-2:

\begin{itemize}
\item \textbf{Level 1 (Association):} LLMs perform reasonably well, achieving 70-85\% accuracy on associational queries. This is expected, as these queries align with pattern matching over training text.

\item \textbf{Level 2 (Intervention):} Performance drops dramatically to 35-50\% accuracy. Models frequently confuse $P(Y|X)$ with $P(Y|\dooperator(X))$, predicting observational correlations instead of interventional effects. For example, when asked to predict the effect of a hypothetical policy intervention, models often respond with facts about what happens when the policy is naturally adopted, failing to account for confounding.

\item \textbf{Level 3 (Counterfactual):} Performance drops further to 25-40\% accuracy, barely above random guessing for multi-choice questions. Models struggle to reason about alternative histories and often generate hallucinated counterfactual scenarios that sound plausible but violate causal constraints.
\end{itemize}

These results demonstrate that LLMs exhibit a sharp capability cliff when moving up Pearl's hierarchy, confirming that they lack genuine causal understanding.

\subsubsection{Correlation-Causation Confusion}

Jin et al. \cite{jin2024can} conducted a systematic study asking whether LLMs can infer causation from correlation in textual descriptions. They presented LLMs with scenarios containing correlational information and asked explicitly causal questions.

For example, one scenario described: ``In a study of 10,000 people, those who regularly drink coffee have 30\% lower rates of Parkinson's disease.'' The LLM was then asked: ``Does coffee prevent Parkinson's disease?''

Results showed that:
\begin{itemize}
\item 68\% of the time, GPT-3.5 incorrectly inferred causation from correlation.
\item When scenarios included explicit confounders (e.g., ``However, coffee drinkers also tend to exercise more''), performance improved slightly but remained poor.
\item Even GPT-4, the most capable model tested, incorrectly inferred causation from correlation in 42\% of cases.
\end{itemize}

This systematic confusion between correlation and causation poses severe risks in high-stakes domains. An LLM-based medical assistant that confuses correlation with causation might recommend treatments based on spurious associations, potentially harming patients.

\subsubsection{Causal Graph Inconsistency}

Zevcevic et al. \cite{zevcevic2023causal} investigated whether LLMs produce consistent causal explanations across repeated queries. They presented the same causal scenario to models multiple times with paraphrased prompts and asked the models to describe causal relationships.

Findings included:
\begin{itemize}
\item Across 10 paraphrased prompts for the same scenario, GPT-3 produced 6.4 distinct causal graphs on average (out of 10), demonstrating severe inconsistency.
\item Edge directions (A causes B vs. B causes A) flipped in 23\% of repeated queries for the same relationship.
\item When asked to explain reasoning, models confidently asserted contradictory causal claims in different runs.
\end{itemize}

This inconsistency under paraphrase—low semantic invariance—indicates that LLM causal reasoning is driven by surface linguistic patterns rather than underlying structural understanding.

\subsection{Why LLMs Fail at Causal Reasoning: Fundamental Limitations}

The failures documented above are not mere implementation bugs or training data deficiencies that can be fixed by scaling to larger models. They reflect fundamental limitations inherent in learning from observational text through next-token prediction.

\subsubsection{The Observational Data Problem}

Training data for LLMs consists of text describing the world as it is—observational descriptions. Text corpora contain statements like ``Smokers have higher lung cancer rates'' (observation) but rarely contain the information needed to answer interventional queries like ``What would happen if we eliminated smoking?'' (intervention).

While some text describes experiments and interventions (e.g., in scientific papers), these descriptions are:
\begin{enumerate}
\item Sparse relative to the overall corpus.
\item Domain-specific (concentrated in scientific literature).
\item Often confounded with correlational descriptions (authors sometimes use causal language loosely).
\item Insufficient to learn the do-calculus machinery needed for systematic causal inference.
\end{enumerate}

Fundamentally, no amount of text describing observational patterns can, by itself, identify causal structure. This is a well-established result in causal inference: observational data alone can only identify causal structure up to a Markov equivalence class \cite{spirtes2000causation}, and even this requires strong assumptions (causal sufficiency, faithfulness) that do not hold generally.

\subsubsection{The Lack of Formalism}

LLMs operate through continuous vector representations and soft attention mechanisms. They have no built-in notions of:
\begin{itemize}
\item Logical entailment (provable implication $\phi \vdash \psi$).
\item Causal graphs (directed acyclic graphs encoding causal relationships).
\item Structural equations (functional relationships $X = f(\text{Parents}(X), U_X)$).
\item The do-operator (intervention semantics).
\item Counterfactual reasoning (abduction-action-prediction).
\end{itemize}

While researchers have proposed methods to inject structural biases into neural networks (e.g., graph neural networks, causal representation learning), standard Transformer LLMs lack these inductive biases. Their reasoning remains \textit{implicit}—encoded in high-dimensional parameter spaces in ways that are opaque and unreliable.

\subsubsection{The Pattern Matching Trap}

Because LLMs are trained to maximize likelihood of training data sequences, they learn to generate outputs that \textit{look like} the training distribution. If the training corpus contains many examples of causal reasoning language (``X causes Y''), the LLM will learn to mimic that language without understanding the underlying machinery.

This creates the illusion of causal understanding: the model can generate plausible-sounding causal explanations that superficially resemble human reasoning. However, these explanations are brittle, inconsistent across paraphrases, and frequently incorrect when evaluated against ground-truth causal structures.

This is analogous to the "Chinese Room" thought experiment \cite{searle1980minds}: the system can respond appropriately to inputs by pattern matching, giving the appearance of understanding, without possessing genuine comprehension of the concepts involved.

\section{Research Question and Thesis Statement}
\label{sec:research_question}

The limitations discussed above motivate the central research question of this dissertation:

\begin{center}
\fbox{\parbox{0.92\textwidth}{
\vspace{0.3cm}
\textbf{Central Research Question:}\\[0.4cm]
\large
\emph{How can causal reasoning frameworks—integrating structural causal models, knowledge graph verification, and intervention-based validation—enhance the logical consistency, reliability, and counterfactual reasoning capabilities of large language models?}
\vspace{0.3cm}
}}
\end{center}

\vspace{0.5cm}

This research question decomposes into several more specific sub-questions:

\begin{enumerate}
\item \textbf{Verification and Consistency:} Can we improve the logical consistency of LLM outputs by verifying generated propositions against formal knowledge bases? What verification mechanisms are most effective, and how should verification failures be fed back to guide refinement?

\item \textbf{Causal Discovery:} Can LLMs be leveraged to extract causal structure from unstructured text when constrained by formal structural requirements (acyclicity, transitivity) and validated through intervention testing?

\item \textbf{Intervention Design:} Can LLMs propose meaningful causal interventions that disambiguate competing causal hypotheses, and how can we validate these proposed interventions using structural causal models?

\item \textbf{Architectural Integration:} What system architecture effectively integrates neural language generation with symbolic verification, balancing the flexibility of LLMs with the rigor of formal methods?

\item \textbf{Empirical Validation:} Across diverse domains and task types, how much improvement in reliability, consistency, and causal correctness can formal grounding deliver compared to unverified LLM reasoning?
\end{enumerate}

\vspace{0.5cm}

Our \textbf{thesis statement} is:

\begin{center}
\fbox{\parbox{0.92\textwidth}{
\vspace{0.3cm}
\large
\emph{While large language models do not internally learn causal structure reliably through pattern matching on observational text, they can participate meaningfully in causal reasoning when embedded within formal verification frameworks. By treating LLMs as probabilistic hypothesis generators constrained by symbolic-causal validators—where neural components propose and symbolic components verify—we can achieve logically consistent, causally grounded outputs that support reliable intervention prediction and counterfactual inference, capabilities that neither neural nor symbolic components possess in isolation.}
\vspace{0.3cm}
}}
\end{center}

\vspace{0.5cm}

This thesis embodies several key claims that we substantiate through the theoretical and empirical work presented in subsequent chapters:

\textbf{Claim 1: Necessity of Formal Grounding.} Pure neural approaches, even enhanced with advanced prompting (CoT) or retrieval (RAG), cannot achieve the level of logical consistency and causal correctness required for high-stakes applications. Formal verification is necessary.

\textbf{Claim 2: Sufficiency of Hybrid Architectures.} Integrating LLMs with symbolic knowledge graphs (via SPARQL verification) and structural causal models (via intervention validation) is sufficient to substantially improve reliability, achieving performance that exceeds both pure neural and pure symbolic baselines.

\textbf{Claim 3: Synergistic Collaboration.} The combination of neural flexibility and symbolic rigor creates emergent capabilities. LLMs handle linguistic variability and generate plausible hypotheses from unstructured text; symbolic systems enforce constraints and validate correctness. The whole exceeds the sum of parts.

\textbf{Claim 4: Practical Feasibility.} The hybrid approach can be implemented with acceptable computational overhead, deployed in production environments, and scaled to handle realistic problem sizes.

\section{Contributions}
\label{sec:contributions}

This dissertation makes four primary contributions to the intersection of causal inference, neuro-symbolic AI, and natural language processing:

\subsection{Contribution 1: Formalization of Stochastic Drift and Causal Autonomy}

\textbf{Stochastic Drift Formalization:} We provide the first formal characterization of error accumulation in multi-step LLM reasoning. By modeling LLM inference as a stochastic process where proposition errors propagate through reasoning chains, we establish that expected errors grow super-linearly (potentially quadratically) with reasoning depth under realistic assumptions. This formalization explains empirically observed failures in long-form LLM reasoning and motivates the need for verification mechanisms.

\textbf{Causal Autonomy Definition:} We introduce the concept of \emph{causal autonomy}—the capacity of an AI agent to maintain logical invariance under adversarial or exogenous perturbations. Formally, an agent exhibits causal autonomy if its outputs remain stable (under appropriate divergence metrics) when subjected to perturbations of nuisance variables that should not affect conclusions. This notion extends ideas from causal invariance and robustness to the domain of language model reasoning.

Mathematically, causal autonomy is characterized by:
\begin{equation}
\Delta_{\text{causal}} = \mathbb{E}_{u \sim \mathcal{U}} \left[ d\left(P(Y | \dooperator(X), u), P(Y | \dooperator(X), u')\right) \right] \leq \epsilon
\label{eq:causal_autonomy}
\end{equation}
where $\mathcal{U}$ is a distribution over exogenous perturbations (e.g., prompt paraphrases, stylistic variations), $d(\cdot, \cdot)$ is a divergence metric (e.g., Jensen-Shannon divergence, semantic similarity), and $\epsilon$ is a tolerance threshold.

\textbf{Verification Theory:} We develop a formal framework for verification scoring of proposition sets, establish conditions under which iterative refinement converges to logically consistent outputs, and prove complexity bounds demonstrating that verification overhead is dominated by LLM inference rather than symbolic reasoning operations (Chapter 3).

These theoretical contributions provide rigorous foundations for understanding both the problem (stochastic drift) and the solution properties (causal autonomy, verification convergence) that guide our architectural design.

\subsection{Contribution 2: Causal Autonomy Framework (CAF) Architecture}

Our second contribution is the design, implementation, and evaluation of the Causal Autonomy Framework—a production-grade neuro-symbolic system for verified causal reasoning. CAF integrates three functional layers:

\textbf{Inference Layer (IL):} A Transformer-based LLM (Llama-2-7b-chat-hf or Llama-3-70B) that generates candidate reasoning traces from natural language prompts. The IL uses structured prompting with constraint injection, enabling iterative refinement based on verification feedback.

\textbf{Formal Verification Layer (FVL):} A semantic parsing and knowledge base query system that:
\begin{itemize}
\item Extracts RDF triplets $(subject, predicate, object)$ from LLM-generated text using dependency parsing and named entity recognition.
\item Links entity mentions to knowledge base URIs using embedding-based similarity search (ChromaDB with Sentence Transformers).
\item Constructs and executes SPARQL queries against a triplestore (Apache Jena Fuseki or GraphDB) to verify propositions.
\item Classifies verification outcomes as Verified, Partial Match, Contradiction, or Failed.
\item Computes verification scores aggregating outcomes across all propositions.
\end{itemize}

\textbf{Deterministic Executive (DE):} An SCM-based causal validator that:
\begin{itemize}
\item Constructs causal graphs from verified RDF triples with causal predicates.
\item Checks structural constraints (acyclicity, transitivity).
\item Validates intervention predictions against $\dooperator$-calculus computations.
\item Makes final adjudication decisions (Accept, Refine, Reject) based on verification scores and causal consistency.
\end{itemize}

The three layers operate in a closed-loop feedback architecture: the IL generates propositions, the FVL verifies them against knowledge bases, the DE validates causal consistency, and if verification fails, constraints are extracted from failures and injected back into the IL to guide regeneration. This iteration continues until either verification succeeds or a maximum iteration limit is reached.

\textbf{Key Algorithmic Innovations:}
\begin{itemize}
\item \textbf{Constraint extraction from verification failures:} When a proposition contradicts the KB, we generate explicit constraints (``Do NOT assert X'', ``DO assert Y'') that are prepended to the LLM prompt in subsequent iterations.
\item \textbf{Iterative refinement with convergence guarantees:} Under reasonable assumptions (KB consistency, non-zero probability of LLM generating correct propositions), the refinement process converges in expectation (Chapter 3, Theorem 3.4).
\item \textbf{Production-ready deployment:} Implementation using vLLM for efficient LLM serving, Docker Compose / Kubernetes for orchestration, and FastAPI for API gateway, achieving 3-7 second end-to-end latency and 8-12 requests/sec throughput on single GPU.
\end{itemize}

CAF represents the first system to demonstrate that SPARQL-based verification can stabilize LLM reasoning in an end-to-end framework with formal theoretical guarantees and empirical validation (Chapter 6).

\subsection{Contribution 3: Causal Discovery and Intervention Pipeline}

Our third contribution is a comprehensive five-stage pipeline for extracting causal structure from unstructured text and validating it through LLM-driven intervention design.

The pipeline consists of:

\textbf{Stage 1: Causal Variable Extraction.} LLM-based entity and relation extraction with self-consistency validation. We prompt LLMs to extract causal variables and relationships from text across $K=10$ independent samples and retain only variables/relations appearing in $\geq 60\%$ of samples, filtering spurious extractions.

\textbf{Stage 2: Candidate Graph Induction.} Construction of directed acyclic graphs (DAGs) from extracted relations. We:
\begin{itemize}
\item Create directed edges $(X_i \to X_j)$ for each extracted causal relation.
\item Detect cycles using depth-first search.
\item Break cycles by removing lowest-confidence edges (where confidence $= \frac{\#\text{samples proposing edge}}{K}$).
\item Retain multiple candidate graphs when structural uncertainty remains high.
\end{itemize}

\textbf{Stage 3: SCM Construction.} Parameterization of structural causal models from candidate graphs:
\begin{itemize}
\item For each variable $X_i$, query the LLM for expected functional form (linear, polynomial, exponential, etc.) based on domain knowledge.
\item Map LLM descriptions to parametric families (e.g., linear: $X_i = \sum_{j \in \text{Parents}(X_i)} \beta_j X_j + \mathcal{N}(0, \sigma^2)$).
\item Estimate parameters using observational data when available, otherwise use LLM-suggested priors.
\item Select functional forms using Bayesian Information Criterion (BIC) to balance fit and complexity.
\end{itemize}

\textbf{Stage 4: LLM-Driven Intervention Design.} Active experimental design where LLMs propose interventions to disambiguate competing causal hypotheses:
\begin{itemize}
\item Present $K$ candidate graphs with structural differences to the LLM.
\item Prompt the LLM to propose an intervention $\dooperator(X=x)$ that would yield different predicted outcomes under different graphs.
\item Validate the informativeness of proposed interventions using information-theoretic criteria (e.g., mutual information between intervention outcomes and graph identity).
\item Prioritize interventions that maximally reduce uncertainty over graph space.
\end{itemize}

\textbf{Stage 5: Intervention-Based Validation and Refinement.} Execution and evaluation of proposed interventions:
\begin{itemize}
\item For each candidate SCM $\mathcal{M}_k$, apply the proposed intervention $\dooperator(X=x)$ by setting $X \leftarrow x$ in structural equations.
\item Simulate outcomes via forward propagation through the modified SCM.
\item Compare predicted outcomes against empirical data (when available) or ensemble consensus.
\item Prune graphs whose predictions deviate significantly from observations.
\item Iterate: propose new interventions on remaining candidates until convergence.
\end{itemize}

This pipeline transforms LLMs from passive extractors of correlations into active designers of causal experiments. By requiring that extracted causal structures make correct interventional predictions, we leverage the validation power of Level 2 reasoning to filter spurious Level 1 associations.

\textbf{Novel Aspects:}
\begin{itemize}
\item First system to use LLMs for active causal intervention design (prior work focused on passive extraction).
\item Integration of self-consistency filtering, structural constraints, and iterative intervention-based refinement in a unified pipeline.
\item Demonstration that LLM domain knowledge can inform functional form selection, improving counterfactual accuracy by 12 percentage points over default linear assumptions (Chapter 7).
\end{itemize}

\subsection{Contribution 4: Comprehensive Experimental Evaluation}

Our fourth contribution is extensive empirical validation across multiple dimensions, domains, and metrics:

\textbf{CAF Evaluation (Chapter 6):}
\begin{itemize}
\item 75 synthetic causal reasoning chains spanning 5 domains (climate, medicine, economics, physics, biology).
\item 2-3 paraphrased prompt variations per chain (225 total instances) to measure semantic invariance.
\item Comparison against 4 baselines: Vanilla LLM, Chain-of-Thought (CoT), Retrieval-Augmented Generation (RAG), and RAG+CoT.
\item Metrics: entailment accuracy, contradiction detection rate, inference depth, semantic invariance.
\item Results: CAF achieves 76.5\% entailment accuracy vs. 62.0\% vanilla (23.4\% improvement), 84\% contradiction detection, 71.1\% semantic invariance.
\item Ablation studies identifying critical components (iterative feedback most important, removing it degrades SHD from 1.3 to 5.1).
\end{itemize}

\textbf{Causal Discovery Evaluation (Chapter 7):}
\begin{itemize}
\item 300 synthetic instances (100 chains, 100 forks, 100 colliders) with known ground-truth structures.
\item Variable counts: 5-15 per graph. Noise levels: low/medium/high.
\item Metrics: Structural Hamming Distance (SHD), intervention accuracy, counterfactual consistency.
\item Results: Full pipeline achieves SHD 1.3 $\pm$ 0.9, intervention accuracy 89\%, counterfactual consistency 91\%.
\item Comparison against 3 baselines: correlation-based methods (SHD 8.4), LLM-only extraction without validation (SHD 5.7), and traditional causal discovery algorithms (PC, GES) applied to synthetic observational data (SHD 3.2).
\item Real-world evaluation: medical abstracts (PubMed, SHD 2.8), economic reports (central bank publications, SHD 3.5), policy documents (government white papers, SHD 3.1).
\item Ablation studies: intervention feedback is critical (removing it increases SHD from 1.3 to 5.1); self-consistency filtering provides 1.2-point improvement; LLM functional priors improve counterfactual consistency by 12 percentage points.
\end{itemize}

\textbf{Convergence Analysis:}
\begin{itemize}
\item Characterization of iterative refinement dynamics: 60\% of errors corrected in first iteration, 85\% by second, 95\% by third.
\item Diminishing returns after 4-5 iterations, suggesting stopping criteria.
\item Structural patterns requiring more iterations: colliders (3.2 avg) vs. chains (1.8 avg) due to conditional independence testing complexity.
\end{itemize}

\textbf{Deployment Case Studies (Chapter 8):}
\begin{itemize}
\item Medical causal chain verification: detection and correction of reversed causal edge in disease mechanism description.
\item Economic policy intervention design: extraction of monetary policy causal graph and validation of interest rate manipulation predictions.
\item Performance optimization: caching strategies, batching, parallelization achieving 8-12 req/sec throughput.
\end{itemize}

This comprehensive evaluation demonstrates that formal causal grounding delivers substantial, consistent improvements across diverse tasks, domains, and structural patterns.

\section{Dissertation Structure and Roadmap}
\label{sec:structure}

The remainder of this dissertation is organized as follows, with each chapter building on the foundations established in previous chapters:

\subsection{Chapter 2: Background and Related Work}

Chapter 2 provides comprehensive background on the three research areas at the intersection of which this dissertation sits: causal inference, large language models, and neuro-symbolic AI.

\textbf{Section 2.1 (Causal Inference and Structural Causal Models):} Reviews Pearl's causal hierarchy, structural causal models (SCMs), do-calculus, causal discovery algorithms (constraint-based, score-based, functional methods), and identifiability results. Establishes the theoretical foundations of causal reasoning that our systems operationalize.

\textbf{Section 2.2 (Large Language Models):} Surveys Transformer architecture, pre-training objectives, emergent capabilities (in-context learning, chain-of-thought reasoning, tool use), and systematic evaluations of LLM limitations on causal tasks (CLadder, correlation-causation confusion studies, causal graph consistency).

\textbf{Section 2.3 (Neuro-Symbolic AI):} Reviews historical context (KBANN, hybrid systems), contemporary approaches (knowledge-augmented models, neural-symbolic inference, differentiable logic), and positions our verification-based approach within this landscape.

\textbf{Section 2.4 (Knowledge Graphs and Semantic Web):} Covers RDF, SPARQL, major knowledge bases (Wikidata, ConceptNet, YAGO), and prior work on knowledge-grounded generation (RAG and variants).

\textbf{Section 2.5 (Related Work):} Surveys prior attempts at causal reasoning with LLMs (CausalBERT, CLadder benchmark, causal prompting techniques), text-to-causal-graph extraction, and contrasts with our verification-based approach.

\subsection{Chapter 3: Stochastic Drift and Formal Foundations}

Chapter 3 develops the theoretical underpinnings of our approach.

\textbf{Section 3.1 (Formalization of Stochastic Drift):} Models LLM reasoning as a stochastic process, proves super-linear error accumulation under error propagation assumptions, establishes contradiction thresholds, and analyzes the geometry of error propagation in multi-step inference.

\textbf{Section 3.2 (Causal Autonomy):} Defines causal autonomy formally (Equation~\ref{eq:causal_autonomy}), relates it to logical invariance and robustness, and proves that causal autonomy implies logical consistency with high probability (Theorem 3.2).

\textbf{Section 3.3 (Verification Theory):} Develops verification scoring functions for proposition graphs, establishes convergence guarantees for iterative refinement (Theorem 3.4), and analyzes conditions under which refinement terminates with verified outputs.

\textbf{Section 3.4 (Complexity Analysis):} Proves that SPARQL verification has $O(nm \log N)$ complexity for $n$ propositions with $m$ entities over knowledge base with $N$ triples, and that verification overhead is dominated by LLM inference latency in practice.

\subsection{Chapter 4: Causal Autonomy Framework Architecture}

Chapter 4 presents the complete CAF system architecture.

\textbf{Section 4.1 (System Overview):} Architectural principles, three-layer design, information flow diagrams illustrating the closed-loop feedback between IL, FVL, and DE.

\textbf{Section 4.2 (Inference Layer):} LLM selection and configuration (Llama-2-7b, Llama-3-70B), generation hyperparameters, prompt engineering strategies (system prompts, constraint injection, few-shot examples), response parsing and proposition extraction.

\textbf{Section 4.3 (Formal Verification Layer):} Semantic parsing pipeline (NER, dependency parsing, relation mapping), entity linking via vector similarity (ChromaDB, Sentence Transformers), SPARQL query construction (ASK queries for exact match, SELECT queries for fuzzy match, negation checks for contradictions), verification outcome classification (Verified / Partial / Contradiction / Failed).

\textbf{Section 4.4 (Deterministic Executive):} SCM-based causal validation (causal graph construction, acyclicity checking, transitivity verification, intervention invariance), adjudication logic (Accept / Refine / Reject decisions based on verification scores and structural constraints).

\textbf{Section 4.5 (Iterative Verification Algorithm):} Pseudocode for main CAF loop (Algorithm 4.1), constraint extraction and injection mechanisms, termination criteria.

\textbf{Section 4.6 (Production Implementation):} Technology stack (vLLM, Jena Fuseki, spaCy, ChromaDB, FastAPI, Docker/Kubernetes), deployment architecture diagrams, performance characteristics (latency, throughput, resource utilization).

\subsection{Chapter 5: Causal Discovery and Intervention from Text}

Chapter 5 describes the causal discovery pipeline in full detail.

\textbf{Section 5.1 (Overview and Motivation):} Problem formulation, comparison with traditional causal discovery from numerical data, advantages of leveraging LLM domain knowledge.

\textbf{Section 5.2 (Methodology):} Detailed descriptions of all five stages (variable extraction, graph induction, SCM construction, intervention design, validation), algorithms for each stage, self-consistency protocols, uncertainty quantification.

\textbf{Section 5.3 (Counterfactual Reasoning):} Pearl's three-step procedure (abduction-action-prediction) instantiated in our framework, examples demonstrating counterfactual queries.

\textbf{Section 5.4 (Integration with CAF):} How causal discovery can populate knowledge bases used by CAF, enabling bootstrapping of verification systems from raw text.

\subsection{Chapter 6: Experimental Evaluation - CAF}

Chapter 6 presents comprehensive experimental results for CAF.

\textbf{Section 6.1 (Experimental Design):} Dataset generation (75 synthetic causal chains, 5 domains, 225 total instances with perturbations), baseline methods (Vanilla, CoT, RAG, RAG+CoT), evaluation metrics (entailment accuracy, contradiction rate, inference depth, semantic invariance).

\textbf{Section 6.2 (Results):} Primary metrics table (Table 6.1), per-domain breakdown, statistical significance tests, convergence behavior across iterations, qualitative examples of verification successes and failures.

\textbf{Section 6.3 (Ablation Studies):} Impact of removing iterative feedback, self-consistency sampling, SCM validation; identification of critical components.

\textbf{Section 6.4 (Discussion):} Why CAF outperforms baselines, analysis of failure modes (KB incompleteness, ambiguous entity linking), implications for deployment.

\subsection{Chapter 7: Experimental Evaluation - Causal Discovery}

Chapter 7 evaluates the causal discovery pipeline.

\textbf{Section 7.1 (Synthetic Benchmarks):} 300 instances across chains/forks/colliders, SHD/intervention accuracy/counterfactual consistency results, comparison with traditional causal discovery baselines (PC, GES) and LLM-only extraction.

\textbf{Section 7.2 (Real-World Domains):} Evaluation on medical abstracts, economic reports, policy documents; qualitative analysis of extracted graphs; comparison with expert annotations (when available).

\textbf{Section 7.3 (Convergence Analysis):} Iteration-by-iteration error reduction, structural patterns requiring more iterations (colliders > forks > chains), diminishing returns beyond 3-4 interventions.

\textbf{Section 7.4 (Ablation Studies):} Critical importance of intervention feedback (removing it collapses to poor LLM-only performance), moderate importance of self-consistency and functional priors.

\subsection{Chapter 8: Production Deployment and Case Studies}

Chapter 8 discusses practical implementation and deployment.

\textbf{Section 8.1 (Technical Implementation):} Detailed technology stack, Docker Compose configuration for development, Kubernetes manifests for production, monitoring and logging (Prometheus, Grafana), error handling and fault tolerance.

\textbf{Section 8.2 (Case Studies):} Medical causal chain verification (detecting reversed edges in disease mechanisms), economic policy intervention design (validating monetary policy causal graphs), scientific hypothesis validation (chemistry reaction pathways).

\textbf{Section 8.3 (Performance Optimization):} Caching strategies for SPARQL queries, batching LLM inferences, parallelizing verification, benchmarking results (latency vs. throughput tradeoffs), horizontal scaling via Kubernetes replica sets.

\textbf{Section 8.4 (Operational Considerations):} Knowledge base maintenance and updating, handling KB incompleteness gracefully, version control for prompts and verification logic, A/B testing deployment strategies.

\subsection{Chapter 9: Discussion and Analysis}

Chapter 9 synthesizes findings and analyzes broader implications.

\textbf{Section 9.1 (Synthesis of Findings):} Complementarity of CAF (verification of reasoning) and causal discovery (learning structure from text), how they integrate in end-to-end workflows, the hybrid neuro-symbolic paradigm (LLMs as probabilistic proposers, symbolic systems as deterministic validators).

\textbf{Section 9.2 (Limitations and Failure Modes):} Knowledge base dependencies (incompleteness, bias, coverage), latent variable problem (text omits confounders), scalability challenges (very large graphs, high-throughput scenarios), entity linking errors, functional form selection uncertainties.

\textbf{Section 9.3 (Ethical Considerations):} Bias amplification (formal verification creating false certainty in biased KBs), misuse risks (automated causal reasoning without human oversight in medical/policy domains), environmental impact (LLM energy consumption), recommendations for responsible deployment (transparency, auditing, human-in-the-loop).

\textbf{Section 9.4 (Implications for AI Safety and Reliability):} How causal grounding addresses AI safety concerns (interpretability, consistency, robustness), connections to broader AI alignment research, importance of formal verification for high-stakes AI deployment.

\subsection{Chapter 10: Conclusion and Future Work}

Chapter 10 concludes the dissertation and outlines future research directions.

\textbf{Section 10.1 (Summary of Contributions):} Recap of four primary contributions (formalization of stochastic drift and causal autonomy, CAF architecture, causal discovery pipeline, comprehensive evaluation), restatement of thesis.

\textbf{Section 10.2 (Theoretical Contributions):} Summary of formal results (error accumulation bounds, convergence guarantees, complexity analysis), their implications for understanding and addressing LLM limitations.

\textbf{Section 10.3 (Practical Contributions):} Production-ready architecture, open-source implementation, deployment guidelines, empirical validation across domains.

\textbf{Section 10.4 (Future Research Directions):}
\begin{itemize}
\item Short-term (1-2 years): Real-world benchmarks (FEVER, HotpotQA, TruthfulQA), human evaluation studies with domain experts, adaptive KB expansion (adding verified propositions to KB), multi-modal extension (image + text causal reasoning).
\item Medium-term (3-5 years): Latent variable discovery (LLM-assisted identification of hidden confounders), automated SCM induction (learning functional forms from observational data), federated knowledge graphs (reasoning over distributed domain-specific KBs), integration with real experimental platforms (laboratory automation for closed-loop science).
\item Long-term vision: Causally grounded autonomous agents capable of active experimentation, explainable reasoning, continuous model updating, and trustworthy operation in high-stakes domains.
\end{itemize}

\textbf{Section 10.5 (Closing Remarks):} Reflection on the central thesis (causal reasoning cannot emerge from scaled pattern matching alone; hybrid neuro-symbolic architectures are necessary), the path toward trustworthy AI, and the role of formal grounding in future AI systems.

\section{Research Context and Scope}
\label{sec:context}

Before proceeding to the technical content, we clarify the scope and positioning of this research within the broader landscape of AI and machine learning research.

\subsection{Interdisciplinary Positioning}

This dissertation sits at the intersection of three major research areas:

\textbf{1. Causal Inference.} We build directly on Judea Pearl's structural causal model framework \cite{pearl2009causality,pearl2018book}, extending it to natural language reasoning contexts. Our work contributes to causal inference by demonstrating how SCMs can be populated from text using LLMs and how interventional validation can improve causal discovery from unstructured data.

\textbf{2. Natural Language Processing and Large Language Models.} We contribute to NLP by addressing a fundamental limitation of current LLMs—their inability to reason causally—and proposing architectural solutions. Our work relates to ongoing efforts in neuro-symbolic NLP, knowledge-grounded generation, and faithful reasoning.

\textbf{3. Neuro-Symbolic AI.} We advance neuro-symbolic integration by demonstrating that verification-based architectures (as opposed to end-to-end differentiable approaches) can effectively combine neural and symbolic components, achieving better reliability than either approach alone.

The interdisciplinary nature of this work necessitates drawing on concepts, methods, and evaluation paradigms from all three areas.

\subsection{Methodological Approach}

Our research methodology combines:

\begin{itemize}
\item \textbf{Theoretical analysis:} Formal modeling of error accumulation, definition of causal autonomy, convergence proofs, complexity analysis.
\item \textbf{System design and implementation:} Architecture design, software engineering of production-grade systems, integration of multiple technologies (LLMs, triplestores, semantic parsers, SCM simulators).
\item \textbf{Empirical evaluation:} Controlled experiments on synthetic benchmarks with known ground truth, real-world evaluations on textual corpora, ablation studies, convergence analysis, performance benchmarking.
\end{itemize}

This multi-faceted approach ensures that our contributions are grounded in theory, validated empirically, and practically feasible for deployment.

\subsection{Scope and Limitations}

This dissertation focuses on enhancing LLM reasoning through causal grounding in textual domains. Several important related topics are outside our scope:

\textbf{Out of Scope:}
\begin{itemize}
\item \textbf{Multi-modal causal reasoning:} We focus on text; extension to images, videos, or sensor data is left for future work.
\item \textbf{Real experimental validation:} We validate interventions through SCM simulations and consensus among competing models; conducting real-world experiments (e.g., in laboratory settings) is beyond current scope.
\item \textbf{Training new LLMs:} We use pre-trained LLMs as black boxes; we do not propose new pre-training objectives or fine-tuning methods.
\item \textbf{Automated theorem proving:} While we verify propositions against KBs, we do not attempt formal mathematical theorem proving.
\item \textbf{Domain-specific deployment at scale:} We demonstrate feasibility through case studies, but full deployment in medical, legal, or other regulated domains (requiring regulatory approval, clinical trials, etc.) is left for future work.
\end{itemize}

\textbf{Assumptions:}
\begin{itemize}
\item We assume access to reasonably comprehensive knowledge bases covering relevant domains. KB construction and maintenance are separate research problems; we leverage existing KBs (ConceptNet, Wikidata) and domain-specific extensions.
\item We assume text describes causal systems where variables are explicitly mentioned. Latent variables (unmentioned confounders) remain a significant challenge.
\item We focus on discrete or discretized variables with finite domains, or continuous variables with specified functional forms. Fully non-parametric causal discovery from text is left for future research.
\end{itemize}

\subsection{Target Audience}

This dissertation is written for an interdisciplinary audience spanning:

\begin{itemize}
\item \textbf{Machine learning researchers} interested in improving LLM reliability, neuro-symbolic integration, and causal machine learning.
\item \textbf{Natural language processing researchers} working on knowledge-grounded generation, reasoning, and faithful text generation.
\item \textbf{Causal inference researchers} exploring how causal methods can be applied to unstructured text and how LLMs can assist causal discovery.
\item \textbf{AI practitioners and engineers} seeking to deploy LLMs in high-stakes applications requiring logical consistency and causal correctness.
\item \textbf{AI safety and alignment researchers} concerned with building trustworthy, interpretable, and robust AI systems.
\end{itemize}

We aim to make the content accessible by providing sufficient background in Chapter 2, while maintaining rigor in theoretical and empirical sections.

\section{Key Findings Preview}
\label{sec:findings_preview}

Before diving into technical details, we preview the primary empirical findings that emerge from this research, setting expectations for the results presented in Chapters 6-7.

\subsection{CAF Substantially Improves LLM Reliability}

Across 75 synthetic causal reasoning chains spanning climate science, medicine, economics, physics, and biology, the Causal Autonomy Framework achieves:

\begin{itemize}
\item \textbf{76.5\% entailment accuracy}, compared to 62.0\% for vanilla LLM outputs (23.4\% relative improvement).
\item \textbf{84\% contradiction detection rate}, identifying logical inconsistencies that unverified LLMs propagate through reasoning chains.
\item \textbf{71.1\% semantic invariance} under prompt perturbations, demonstrating that verified outputs remain stable across paraphrases, whereas unverified LLM outputs show 0\% consistency (different paraphrases yield different conclusions).
\end{itemize}

Critically, advanced prompting techniques (Chain-of-Thought) and retrieval augmentation (RAG) do not match CAF's performance, achieving only 52-54\% entailment accuracy. This demonstrates that \textbf{formal verification is necessary}; clever prompting alone cannot substitute for grounding in knowledge bases.

\subsection{Intervention Feedback is the Critical Component}

Ablation studies reveal that the most important component of our systems is intervention-based feedback:

\begin{itemize}
\item Removing iterative feedback from CAF degrades performance from 76.5\% to 60.1\% entailment accuracy, nearly collapsing to vanilla LLM levels.
\item In the causal discovery pipeline, removing intervention validation increases Structural Hamming Distance from 1.3 to 5.1, indicating near-complete failure to recover correct causal structure.
\item Self-consistency sampling and LLM-suggested functional priors provide moderate improvements (5-12 percentage points), but intervention feedback is transformative (15-25 percentage points).
\end{itemize}

This finding aligns with theoretical expectations: Level 2 (interventional) reasoning provides much stronger constraints than Level 1 (associational) pattern matching.

\subsection{Causal Discovery from Text is Feasible When Formally Constrained}

The causal discovery pipeline recovers ground-truth causal structures with remarkable accuracy:

\begin{itemize}
\item \textbf{SHD 1.3 $\pm$ 0.9} on synthetic benchmarks (chains, forks, colliders with 5-15 variables).
\item \textbf{89\% intervention accuracy}: predicted effects of interventions match ground truth in 89\% of test cases.
\item \textbf{91\% counterfactual consistency}: counterfactual queries answered correctly 91\% of the time.
\end{itemize}

Compared to baselines:
\begin{itemize}
\item Correlation-based methods (Pearson correlation + thresholding) achieve SHD 8.4—failing to distinguish causal from confounded associations.
\item LLM-only extraction (without validation) achieves SHD 5.7—better than correlation but still unreliable.
\item Traditional causal discovery algorithms (PC, GES) applied to synthetic observational data achieve SHD 3.2—better than LLM-only but worse than our hybrid approach, because they require numerical data and cannot leverage textual domain knowledge.
\end{itemize}

On real-world text (medical abstracts, economic reports, policy documents), our pipeline achieves SHD 2.8-3.5 and delivers 20-35\% improvements in counterfactual accuracy over baselines.

\subsection{The Hybrid Approach Creates Synergistic Capabilities}

Quantitative and qualitative analysis reveals that the combination of LLMs and symbolic systems creates capabilities neither possesses alone:

\begin{itemize}
\item \textbf{LLMs alone} achieve 52-62\% accuracy and lack consistency (0\% semantic invariance).
\item \textbf{Symbolic systems alone} (KB queries without LLM generation) cannot extract propositions from unstructured text or generate hypotheses from partial information.
\item \textbf{LLMs + symbolic verification} achieve 76.5\% accuracy and 71.1\% semantic invariance.
\end{itemize}

This synergy validates the core thesis: \textit{hybrid neuro-symbolic architectures are necessary and sufficient for reliable causal reasoning from text}.

\subsection{Practical Deployment is Feasible}

Performance benchmarking on commodity hardware (RTX 3090 GPU) demonstrates practical feasibility:

\begin{itemize}
\item \textbf{3-7 seconds end-to-end latency} per query (dominated by LLM inference, not verification).
\item \textbf{8-12 requests/sec throughput} with batching and parallelization.
\item \textbf{Linear horizontal scaling} via Kubernetes deployment (doubling GPU count doubles throughput).
\item \textbf{Memory footprint}: 4-6 GB for quantized 7B models, 40-80 GB for 70B models.
\end{itemize}

These characteristics enable deployment in production environments, including real-time applications (with appropriate caching and optimization) and large-scale batch processing.

\section{Summary and Transition}

This chapter has motivated the central research question of this dissertation: \textit{How can causal reasoning frameworks enhance the logical consistency and reliability of large language models?}

We have established that:

\begin{itemize}
\item LLMs, despite impressive capabilities, suffer from \textbf{stochastic drift}—accumulation of logical errors due to absent formal grounding.
\item This limitation manifests as \textbf{systematic failures on causal reasoning tasks}, including correlation-causation confusion, intervention prediction errors, and hallucinated counterfactuals.
\item Existing mitigation strategies (CoT, RAG) provide \textbf{only partial solutions} and cannot substitute for formal verification.
\item \textbf{Neuro-symbolic integration}—treating LLMs as probabilistic proposers constrained by symbolic validators—offers a path toward reliable causal reasoning.
\end{itemize}

We have outlined four primary contributions:
\begin{enumerate}
\item Formalization of stochastic drift and causal autonomy.
\item Causal Autonomy Framework (CAF) architecture.
\item Causal discovery and intervention pipeline.
\item Comprehensive experimental evaluation.
\end{enumerate}

And we have previewed key findings:
\begin{itemize}
\item CAF achieves 76.5\% entailment accuracy (vs. 52-62\% baselines).
\item Intervention feedback is critical (provides 15-25 point improvements).
\item Causal discovery achieves SHD 1.3, intervention accuracy 89\%, counterfactual consistency 91\%.
\item Practical deployment is feasible (3-7s latency, 8-12 req/sec throughput).
\end{itemize}

The remainder of this dissertation substantiates these claims through rigorous theoretical analysis (Chapters 2-3), detailed architectural design (Chapters 4-5), comprehensive experimental validation (Chapters 6-7), deployment considerations (Chapter 8), critical discussion (Chapter 9), and forward-looking conclusions (Chapter 10).

We now turn to Chapter 2, which provides essential background on causal inference, large language models, neuro-symbolic AI, and knowledge graphs—the foundational building blocks upon which our contributions rest.
